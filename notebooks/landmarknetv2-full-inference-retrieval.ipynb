{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404b5b4f",
   "metadata": {
    "papermill": {
     "duration": 0.032693,
     "end_time": "2021-09-29T20:02:17.606589",
     "exception": false,
     "start_time": "2021-09-29T20:02:17.573896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da424438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:02:17.679024Z",
     "iopub.status.busy": "2021-09-29T20:02:17.677096Z",
     "iopub.status.idle": "2021-09-29T20:02:17.755960Z",
     "shell.execute_reply": "2021-09-29T20:02:17.755244Z",
     "shell.execute_reply.started": "2021-09-29T18:18:05.073454Z"
    },
    "papermill": {
     "duration": 0.115813,
     "end_time": "2021-09-29T20:02:17.756140",
     "exception": false,
     "start_time": "2021-09-29T20:02:17.640327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "class Config:\n",
    "    batch_size = 64\n",
    "    num_workers = 2\n",
    "    num_to_rerank = 100\n",
    "    seed = 717171\n",
    "    embed_dim = 1024\n",
    "    CACHE_DIR = '/kaggle/temp/'\n",
    "    \n",
    "hyperparameters = Config()\n",
    "os.makedirs(hyperparameters.CACHE_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80baebec",
   "metadata": {
    "papermill": {
     "duration": 0.031068,
     "end_time": "2021-09-29T20:02:17.817847",
     "exception": false,
     "start_time": "2021-09-29T20:02:17.786779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92b8991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:02:17.900208Z",
     "iopub.status.busy": "2021-09-29T20:02:17.889107Z",
     "iopub.status.idle": "2021-09-29T20:03:22.656106Z",
     "shell.execute_reply": "2021-09-29T20:03:22.655517Z",
     "shell.execute_reply.started": "2021-09-29T18:18:05.169406Z"
    },
    "papermill": {
     "duration": 64.807334,
     "end_time": "2021-09-29T20:03:22.656283",
     "exception": false,
     "start_time": "2021-09-29T20:02:17.848949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pkg-wheels/einops-0.3.2-py3-none-any.whl\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.3.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/pkg-wheels/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from faiss-gpu==1.6.3) (1.19.5)\r\n",
      "Installing collected packages: faiss-gpu\r\n",
      "Successfully installed faiss-gpu-1.6.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/pkg-wheels/einops-0.3.2-py3-none-any.whl\n",
    "!pip install ../input/pkg-wheels/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl\n",
    "!cp -r ../input/loftr-repo/ /kaggle/temp/\n",
    "sys.path.append('/kaggle/temp/loftr-repo/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4b16d",
   "metadata": {
    "papermill": {
     "duration": 0.034466,
     "end_time": "2021-09-29T20:03:22.725704",
     "exception": false,
     "start_time": "2021-09-29T20:03:22.691238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5368eaa",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:22.809369Z",
     "iopub.status.busy": "2021-09-29T20:03:22.808266Z",
     "iopub.status.idle": "2021-09-29T20:03:32.345584Z",
     "shell.execute_reply": "2021-09-29T20:03:32.346139Z",
     "shell.execute_reply.started": "2021-09-29T18:19:05.881872Z"
    },
    "papermill": {
     "duration": 9.585403,
     "end_time": "2021-09-29T20:03:32.346334",
     "exception": false,
     "start_time": "2021-09-29T20:03:22.760931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "from scipy import spatial\n",
    "import pydegensac\n",
    "import copy\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn. model_selection import KFold, StratifiedKFold\n",
    "import torch, cv2\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings, math\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.nn.parameter import Parameter\n",
    "import albumentations as A\n",
    "import timm, gc\n",
    "from sklearn.cluster import DBSCAN as dbscan\n",
    "from src.loftr import LoFTR, default_cfg\n",
    "import csv, shutil, glob, pickle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_torch(seed=hyperparameters.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f837278",
   "metadata": {
    "papermill": {
     "duration": 0.035658,
     "end_time": "2021-09-29T20:03:32.418877",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.383219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d006c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:32.517477Z",
     "iopub.status.busy": "2021-09-29T20:03:32.497971Z",
     "iopub.status.idle": "2021-09-29T20:03:32.520322Z",
     "shell.execute_reply": "2021-09-29T20:03:32.519783Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.508077Z"
    },
    "papermill": {
     "duration": 0.066798,
     "end_time": "2021-09-29T20:03:32.520477",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.453679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self,image_ids, mode = 'train', prob_type = 'retrieval'):\n",
    "        self.image_ids = image_ids\n",
    "        \n",
    "        if mode in ('train', 'index', 'test'):\n",
    "#             self.file_path = f'../input/landmark-recognition-2021/{mode}'\n",
    "            self.file_path = f'../input/landmark-{prob_type}-2021/{mode}'\n",
    "        elif mode == 'nolandmark':\n",
    "            self.file_path = f'../input/google-landmark-2021-validation/valid'\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "        self.transform = self._build_augmentation()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_     = self.image_ids[idx]\n",
    "        file_path = f'{self.file_path}/{file_[0]}/{file_[1]}/{file_[2]}/{file_}.jpg'\n",
    "        raw_image = cv2.imread(file_path)\n",
    "        img       = self.process_img(cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB))\n",
    "        img       = img.transpose(2,0,1)\n",
    "        return torch.from_numpy(img)\n",
    "    \n",
    "    def process_img(self, img):\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image = img)['image']\n",
    "        img = img.astype(np.float32)\n",
    "        img = self.normalize(img)\n",
    "        return img\n",
    "        \n",
    "    def normalize(self, img):\n",
    "        mean = np.array([123.675, 116.28 , 103.53 ], dtype=np.float32)\n",
    "        std = np.array([58.395   , 57.120, 57.375   ], dtype=np.float32)\n",
    "        img = img.astype(np.float32)\n",
    "        img -= mean\n",
    "        img *= np.reciprocal(std, dtype=np.float32)\n",
    "        return img\n",
    "    \n",
    "    def _build_augmentation(self):\n",
    "        return  A.Compose([\n",
    "                A.SmallestMaxSize(512),\n",
    "                A.CenterCrop(height=448,width=448,p=1.)\n",
    "            ])\n",
    "    \n",
    "class GLRTRFDataset(Dataset):\n",
    "    def __init__(self, feat, pair_tuples):\n",
    "        self.feat = feat\n",
    "        self.pair_tuples = pair_tuples\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        sample      = self.pair_tuples[index]\n",
    "        query_emb   = self.feat[sample[0][0]:sample[0][0]+1]\n",
    "        gallary_emb = self.feat[sample[0][1]:sample[0][1]+1]\n",
    "        features    = np.expand_dims(np.array(sample[1]), 0)\n",
    "        label       = np.expand_dims(np.array(sample[2]), 0)\n",
    "        del sample\n",
    "        return query_emb, gallary_emb, features, label\n",
    "    \n",
    "    def __len__(self,):\n",
    "        return len(self.pair_tuples)\n",
    "    \n",
    "class GraphDataset(Dataset):\n",
    "\n",
    "    def __init__(self, feats=None, labels=None, weights=None, pair_tuples=None, k=50, top_neighbors=None):\n",
    "        self.feats = feats\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.pair_tuples = pair_tuples\n",
    "        self.k = k\n",
    "        self.top_neighbors = top_neighbors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        i, j = self.pair_tuples[index]\n",
    "        feat = torch.FloatTensor(self.feats[i][j])\n",
    "\n",
    "        padding_i = [[0] * feat.shape[0]] * (self.k - len(self.top_neighbors[i]))\n",
    "        neighbor_feats_i = torch.FloatTensor([\n",
    "            self.feats[i][neighbor]\n",
    "            for neighbor in self.top_neighbors[i]\n",
    "        ] + padding_i)\n",
    "        padding_j = [[0] * feat.shape[0]] * (self.k - len(self.top_neighbors[j]))\n",
    "        neighbor_feats_j = torch.FloatTensor([\n",
    "            self.feats[j][neighbor]\n",
    "            for neighbor in self.top_neighbors[j]\n",
    "        ] + padding_j)\n",
    "        neighbor_feats = torch.cat([feat.unsqueeze(0), neighbor_feats_i, neighbor_feats_j], dim=0)\n",
    "\n",
    "        outputs = (feat, neighbor_feats)\n",
    "        if self.labels is not None:\n",
    "            outputs += (self.labels[i] == self.labels[j],)\n",
    "        if self.weights is not None:\n",
    "            outputs += (self.weights[i],)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f868da6",
   "metadata": {
    "papermill": {
     "duration": 0.033807,
     "end_time": "2021-09-29T20:03:32.588116",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.554309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7670c9",
   "metadata": {
    "papermill": {
     "duration": 0.034111,
     "end_time": "2021-09-29T20:03:32.656825",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.622714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metric Learning Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1bbd162",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:32.745310Z",
     "iopub.status.busy": "2021-09-29T20:03:32.742969Z",
     "iopub.status.idle": "2021-09-29T20:03:32.746140Z",
     "shell.execute_reply": "2021-09-29T20:03:32.746668Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.535186Z"
    },
    "papermill": {
     "duration": 0.057039,
     "end_time": "2021-09-29T20:03:32.746818",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.689779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        return cosine\n",
    "    \n",
    "def l2_norm(input, axis = 1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "\n",
    "    return output\n",
    "    \n",
    "class CurricularFace(nn.Module):\n",
    "    r\"\"\"Implement of CurricularFace (https://arxiv.org/pdf/2004.00288.pdf):\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        device_id: the ID of GPU where the model will be trained by model parallel. \n",
    "                       if device_id=None, it will be trained on CPU without model parallel.\n",
    "        m: margin\n",
    "        s: scale of outputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, m=0.4, s=45.0):\n",
    "        super(CurricularFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.threshold = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "        self.kernel = Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.register_buffer(\"t\", torch.zeros(1))\n",
    "        nn.init.normal_(self.kernel, std=0.01)\n",
    "\n",
    "    def forward(self, embbedings, label = None):\n",
    "        embbedings = l2_norm(embbedings, axis=1)\n",
    "        kernel_norm = l2_norm(self.kernel, axis=0)\n",
    "        cos_theta = torch.mm(embbedings, kernel_norm)\n",
    "#         cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             origin_cos = cos_theta.clone()\n",
    "#         target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n",
    "\n",
    "#         sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
    "#         cos_theta_m = (\n",
    "#             target_logit * self.cos_m - sin_theta * self.sin_m\n",
    "#         )  # cos(target+margin)\n",
    "#         mask = cos_theta > cos_theta_m\n",
    "#         final_target_logit = torch.where(\n",
    "#             target_logit > self.threshold, cos_theta_m, target_logit - self.mm\n",
    "#         )\n",
    "#         hard_example = cos_theta[mask]\n",
    "#         with torch.no_grad():\n",
    "#             self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t\n",
    "#         cos_theta[mask] = hard_example * (self.t + hard_example)\n",
    "#         cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n",
    "#         output = cos_theta * self.s\n",
    "        return cos_theta\n",
    "\n",
    "    \n",
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.reset_parameters()\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f60d2",
   "metadata": {
    "papermill": {
     "duration": 0.033907,
     "end_time": "2021-09-29T20:03:32.815099",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.781192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8d4f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:32.910440Z",
     "iopub.status.busy": "2021-09-29T20:03:32.908066Z",
     "iopub.status.idle": "2021-09-29T20:03:32.911246Z",
     "shell.execute_reply": "2021-09-29T20:03:32.911817Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.879746Z"
    },
    "papermill": {
     "duration": 0.062512,
     "end_time": "2021-09-29T20:03:32.911980",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.849468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)[:,:,0,0]\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + f\"(p={self.p.data.tolist()[0]:.4f}, eps={str(self.eps)})\"\n",
    "        )\n",
    "\n",
    "class GridGeM(nn.Module):\n",
    "    def __init__(self, p = 3, eps = 1e-6):\n",
    "        super(GridGeM, self).__init__()\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        f_a, f_b = torch.split(x,[W//2, W//2],dim=-1)\n",
    "        f_a, f_c = torch.split(f_a,[H//2, H//2],dim=-2)\n",
    "        f_b, f_d = torch.split(f_b,[H//2, H//2],dim=-2)\n",
    "\n",
    "        feats = torch.cat([gem(f_a, p=self.p, eps=self.eps),gem(f_b, p=self.p, eps=self.eps),gem(f_c, p=self.p, eps=self.eps),gem(f_d, p=self.p, eps=self.eps),], axis=1)\n",
    "        \n",
    "        return feats\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + f\"(p={self.p.data.tolist()[0]:.4f}, eps={str(self.eps)})\"\n",
    "        )\n",
    "    \n",
    "class GridGeMCosdist(nn.Module):\n",
    "    def __init__(self, p = 3, eps = 1e-6):\n",
    "        super(GridGeMCosdist, self).__init__()\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.neck = nn.AdaptiveAvgPool2d( (1,1) )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        f_a, f_b = torch.split(x,[W//2, W//2],dim=-1)\n",
    "        f_a, f_c = torch.split(f_a,[H//2, H//2],dim=-2)\n",
    "        f_b, f_d = torch.split(f_b,[H//2, H//2],dim=-2)\n",
    "        f_a = f_a.mean(dim=-1).softmax(dim=-1)\n",
    "        f_b = f_b.mean(dim=-1).softmax(dim=-1)\n",
    "        f_c = f_c.mean(dim=-1).softmax(dim=-1)\n",
    "        f_d = f_d.mean(dim=-1).softmax(dim=-1)\n",
    "\n",
    "        feats = torch.stack([(1.0 - F.cosine_similarity(f_a, f_b, dim=-1)), (1.0 - F.cosine_similarity(f_a, f_c, dim=-1)),\n",
    "                           (1.0 - F.cosine_similarity(f_a, f_d, dim=-1)), (1.0 - F.cosine_similarity(f_b, f_c, dim=-1)),\n",
    "                          (1.0 - F.cosine_similarity(f_b, f_d, dim=-1)), (1.0 - F.cosine_similarity(f_c, f_d, dim=-1))]).view(-1,1)\n",
    "        \n",
    "        return feats\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + f\"(p={self.p.data.tolist()[0]:.4f}, eps={str(self.eps)})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cd148",
   "metadata": {
    "papermill": {
     "duration": 0.034719,
     "end_time": "2021-09-29T20:03:32.981040",
     "exception": false,
     "start_time": "2021-09-29T20:03:32.946321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a57fb5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:33.066071Z",
     "iopub.status.busy": "2021-09-29T20:03:33.065156Z",
     "iopub.status.idle": "2021-09-29T20:03:33.069664Z",
     "shell.execute_reply": "2021-09-29T20:03:33.069103Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.90362Z"
    },
    "papermill": {
     "duration": 0.053879,
     "end_time": "2021-09-29T20:03:33.069826",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.015947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LandmarkNet(nn.Module):\n",
    "    def __init__(self, out_feature, backbone='tf_efficientnet_b6_ns',  pretrained=True, pool_type = 'gem',metric ='arcface', sub_center = False):\n",
    "        super(LandmarkNet, self).__init__()\n",
    "        self.backbne_name = backbone\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
    "        self.out_feature = out_feature\n",
    "\n",
    "        if \"efficientnet\" in backbone:\n",
    "            self.in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif \"nfnet\" in backbone:\n",
    "            self.in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "        elif 'resnet' in backbone:\n",
    "            self.in_features = self.backbone.fc.in_features\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.fc = nn.Identity() \n",
    "            \n",
    "        self.out_feature = out_feature\n",
    "        \n",
    "        if pool_type == 'gem':\n",
    "            self.pooling =  GeM()\n",
    "        elif pool_type == 'gridgem':\n",
    "            self.pooling =  GridGeM()\n",
    "            self.in_features *= 4\n",
    "        elif pool_type == 'gridgemcosdist':\n",
    "            self.pooling =  GridGeMCosdist()\n",
    "            self.in_features *= 6\n",
    "        else:\n",
    "            self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.neck = nn.Sequential(\n",
    "                nn.Linear(self.in_features, 512, bias=True),\n",
    "                nn.BatchNorm1d(512),\n",
    "                torch.nn.PReLU()\n",
    "            )\n",
    "        if metric == 'arcface':\n",
    "            if sub_center:\n",
    "                self.final = ArcMarginProduct_subcenter(512, self.out_feature)\n",
    "            else:\n",
    "                self.final = ArcMarginProduct(512, self.out_feature)\n",
    "        elif metric == 'curricular':\n",
    "            self.final = CurricularFace(512, self.out_feature)\n",
    "\n",
    "    def forward(self, x, get_embeddings = True):\n",
    "        if \"efficientnet\" in self.backbne_name or \"nfnet\" in self.backbne_name:\n",
    "            batch_size = x.shape[0]\n",
    "            features = self.backbone(x)\n",
    "            features = self.pooling(features).view(batch_size, -1)\n",
    "            features = self.neck(features)\n",
    "            features = F.normalize(features)\n",
    "\n",
    "        if not get_embeddings:\n",
    "            return self.final(features)\n",
    "        return features, self.final(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcb19d",
   "metadata": {
    "papermill": {
     "duration": 0.033092,
     "end_time": "2021-09-29T20:03:33.136539",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.103447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70a4a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:33.231435Z",
     "iopub.status.busy": "2021-09-29T20:03:33.229181Z",
     "iopub.status.idle": "2021-09-29T20:03:33.234529Z",
     "shell.execute_reply": "2021-09-29T20:03:33.233954Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.92076Z"
    },
    "papermill": {
     "duration": 0.0641,
     "end_time": "2021-09-29T20:03:33.234666",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.170566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=200):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "\n",
    "class GLRTRFModel(nn.Module):\n",
    "    def __init__(self, embed_dim=128, dropout_rate=0.2):\n",
    "        super(GLRTRFModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cat = nn.Sequential(\n",
    "            nn.Linear(embed_dim*2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        ) \n",
    "        self.cat1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim*2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        ) \n",
    "        \n",
    "        self.transformer = nn.Transformer(nhead=8, d_model = embed_dim, num_encoder_layers= N_LAYER, num_decoder_layers= N_LAYER, dropout = dropout_rate)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim) \n",
    "\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        \n",
    "        self.mlp1 = nn.Linear(N_FEATURE, 1024)\n",
    "        self.mlp2 = nn.ReLU()\n",
    "        self.mlp3 = nn.Linear(1024, 512)\n",
    "        self.mlp4 = nn.ReLU()\n",
    "        self.mlp5 = nn.Linear(512, embed_dim)\n",
    "        self.pred = nn.Linear(embed_dim*2, 1)\n",
    "    \n",
    "    def forward(self, query_embedding, gallary_embedding, features):\n",
    "        device = query_embedding.device \n",
    "        \n",
    "        x1 = self.mlp1(features)\n",
    "        x1 = self.mlp2(x1)\n",
    "        x1 = self.mlp3(x1)\n",
    "        x1 = self.mlp4(x1)\n",
    "        x1 = self.mlp5(x1)\n",
    "\n",
    "        x = torch.cat([query_embedding,x1], axis=-1)\n",
    "        x = self.cat(x)\n",
    "        e = torch.cat([gallary_embedding, x1], axis=-1)\n",
    "        e = self.cat1(e)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        e = e.permute(1, 0, 2)\n",
    "        \n",
    "        att_mask = future_mask(x.size(0)).to(device)\n",
    "        att_output = self.transformer( x,e, src_mask=att_mask, tgt_mask=att_mask, memory_mask = att_mask)\n",
    "        att_output = self.layer_normal(att_output+e)\n",
    "        att_output = att_output.permute(1, 0, 2)\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.layer_normal(x + att_output)\n",
    "        x = torch.cat([x,x1], axis = -1)\n",
    "        x = self.pred(x)\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ef01d",
   "metadata": {
    "papermill": {
     "duration": 0.03475,
     "end_time": "2021-09-29T20:03:33.303831",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.269081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7428a06e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:33.387253Z",
     "iopub.status.busy": "2021-09-29T20:03:33.375279Z",
     "iopub.status.idle": "2021-09-29T20:03:33.399928Z",
     "shell.execute_reply": "2021-09-29T20:03:33.399393Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.945719Z"
    },
    "papermill": {
     "duration": 0.061186,
     "end_time": "2021-09-29T20:03:33.400083",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.338897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h):\n",
    "        Wh = h @ self.W  # h.shape: (B, N, in_features), Wh.shape: (B, N, out_features)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(3))\n",
    "\n",
    "        attention = F.softmax(e, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        B, N, D = Wh.shape\n",
    "\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=1)\n",
    "        Wh_repeated_alternating = Wh.repeat(1, N, 1)\n",
    "\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=2)\n",
    "        return all_combinations_matrix.view(-1, N, N, 2 * D)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GATPairClassifier(nn.Module):\n",
    "    def __init__(self, nfeat, nhid=8, nclass=1, dropout=0.6, alpha=0.2, nheads=8, pooling='first'):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nhid, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(nfeat + nhid, nhid),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(nhid),\n",
    "            nn.Linear(nhid, nclass),\n",
    "        )\n",
    "\n",
    "    def forward_gat(self, x):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x) for att in self.attentions], dim=2)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x))\n",
    "        if self.pooling == 'first':\n",
    "            return x[:, 0]\n",
    "        elif self.pooling == 'mean':\n",
    "            return x.mean(dim=1)\n",
    "\n",
    "    def forward(self, feats, neighbor_feats):\n",
    "        gat_feats = self.forward_gat(neighbor_feats)\n",
    "        cat_feats = torch.cat([feats, gat_feats], dim=1)\n",
    "        return self.classifier(cat_feats).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cca4d",
   "metadata": {
    "papermill": {
     "duration": 0.034148,
     "end_time": "2021-09-29T20:03:33.468831",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.434683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d222ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:33.544106Z",
     "iopub.status.busy": "2021-09-29T20:03:33.543256Z",
     "iopub.status.idle": "2021-09-29T20:03:33.546874Z",
     "shell.execute_reply": "2021-09-29T20:03:33.547391Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.970171Z"
    },
    "papermill": {
     "duration": 0.044346,
     "end_time": "2021-09-29T20:03:33.547569",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.503223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_total_score(global_score, local_score, transformer_score, gcn_score, lgb_score, weights = [1]*5):\n",
    "    classifier_score = (transformers_score * weights[2] + gcn_score * weights[3] + lgb_score * weights[4])\n",
    "    score = (global_score * classifier_score * weights[0]) + ((local_score/MAX_INLIER_SCORE) * classifier_score * weights[0])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a36b37",
   "metadata": {
    "papermill": {
     "duration": 0.035048,
     "end_time": "2021-09-29T20:03:33.617509",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.582461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4640b86e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:33.693785Z",
     "iopub.status.busy": "2021-09-29T20:03:33.692674Z",
     "iopub.status.idle": "2021-09-29T20:03:33.696002Z",
     "shell.execute_reply": "2021-09-29T20:03:33.695419Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.97958Z"
    },
    "papermill": {
     "duration": 0.044629,
     "end_time": "2021-09-29T20:03:33.696138",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.651509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labelmap(TRAIN_LABELMAP_PATH):\n",
    "    with open(TRAIN_LABELMAP_PATH, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        labelmap = {row['id']: row['landmark_id'] for row in csv_reader}\n",
    "    gc.collect()\n",
    "    return labelmap\n",
    "\n",
    "def trf_data():\n",
    "    pass\n",
    "\n",
    "def gcn_data():\n",
    "    pass\n",
    "\n",
    "def lgb_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433770e5",
   "metadata": {
    "papermill": {
     "duration": 0.033754,
     "end_time": "2021-09-29T20:03:33.766836",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.733082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60746762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:33.852458Z",
     "iopub.status.busy": "2021-09-29T20:03:33.850145Z",
     "iopub.status.idle": "2021-09-29T20:03:33.853235Z",
     "shell.execute_reply": "2021-09-29T20:03:33.853776Z",
     "shell.execute_reply.started": "2021-09-29T18:19:14.988727Z"
    },
    "papermill": {
     "duration": 0.053167,
     "end_time": "2021-09-29T20:03:33.853957",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.800790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_submission_csv(DATASET_DIR, test_ids = None, predictions=None, mode = 'recognition', submit_fname = 'submission.csv'):\n",
    "    \"\"\"Saves optional `predictions` as submission.csv.\n",
    "\n",
    "      The csv has columns {id, landmarks}. The landmarks column is a string\n",
    "      containing the label and score for the id, separated by a ws delimeter.\n",
    "\n",
    "      If `predictions` is `None` (default), submission.csv is copied from\n",
    "      sample_submission.csv in `IMAGE_DIR`.\n",
    "\n",
    "      Args:\n",
    "        predictions: Optional dict of image ids to dicts with keys {class, score}.\n",
    "      \"\"\"\n",
    "    print('Submitting ..... ')\n",
    "    if mode == 'retrieval':\n",
    "        predicitions = defaultdict(list)\n",
    "        for ix, i in enumerate(predictions):\n",
    "            tmp = []\n",
    "            for j in range(len(i)):\n",
    "                tmp.append(i[j][0])\n",
    "            predicitions[test_ids[ix]] = tmp\n",
    "    elif mode == 'recognition':\n",
    "        predicitions = defaultdict(lambda: defaultdict())\n",
    "        for ix, i in enumerate(predictions):\n",
    "            tmp = {}\n",
    "            for j in range(len(i)):\n",
    "                tmp['class'] = i[j][1]\n",
    "                tmp['score'] = i[j][2]\n",
    "            predicitions[test_ids[ix]] = tmp\n",
    "            \n",
    "    gc.collect()\n",
    "    if predictions is None:\n",
    "    # Dummy submission!\n",
    "        shutil.copyfile(\n",
    "            os.path.join(DATASET_DIR, 'sample_submission.csv'), 'submission.csv')\n",
    "        return\n",
    "    \n",
    "    if mode == 'recognition':\n",
    "        with open(submit_fname, 'w') as submission_csv:\n",
    "            csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'landmarks'])\n",
    "            csv_writer.writeheader()\n",
    "            for ix,(image_id, prediction) in enumerate(predicitions.items()):\n",
    "                label = prediction['class']\n",
    "                score = prediction['score']\n",
    "                csv_writer.writerow({'id': image_id, 'landmarks': f'{label} {score}'})\n",
    "        gc.collect()\n",
    "        return submit_fname\n",
    "    else:\n",
    "        with open(submit_fname, 'w') as submission_csv:\n",
    "            csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'images'])\n",
    "            csv_writer.writeheader()\n",
    "            for image_id, prediction in predicitions.items():\n",
    "                image_ids = prediction\n",
    "                csv_writer.writerow({'id': image_id, 'images': ' '.join(image_ids)})\n",
    "        gc.collect()\n",
    "        return submit_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2f785",
   "metadata": {
    "papermill": {
     "duration": 0.033947,
     "end_time": "2021-09-29T20:03:33.922543",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.888596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Global Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50cee787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:34.003461Z",
     "iopub.status.busy": "2021-09-29T20:03:34.002581Z",
     "iopub.status.idle": "2021-09-29T20:03:34.006718Z",
     "shell.execute_reply": "2021-09-29T20:03:34.006156Z",
     "shell.execute_reply.started": "2021-09-29T18:19:15.006055Z"
    },
    "papermill": {
     "duration": 0.04889,
     "end_time": "2021-09-29T20:03:34.006851",
     "exception": false,
     "start_time": "2021-09-29T20:03:33.957961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_global_features(df, mode, prob_type, return_probs = False):\n",
    "    dataset     = LandmarkDataset(df['id'].values, mode = mode, prob_type = prob_type)\n",
    "    dataloader  = DataLoader(dataset, batch_size = hyperparameters.batch_size, shuffle=False, num_workers=hyperparameters.num_workers)\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(dataloader):\n",
    "            img  = img.cuda()\n",
    "            feat = []\n",
    "            for model in models:\n",
    "                feat_m,_  = model(img)\n",
    "                feat.append(feat_m)\n",
    "            feat = torch.cat(feat,dim=1) \n",
    "            \n",
    "#             if return_probs:\n",
    "#                 probs_m = (lo_1 + lo_2)/2\n",
    "#                 save_pickle(probs_m,f'{mode}_probs.pkl')\n",
    "                \n",
    "            feats.append(feat.detach().cpu())\n",
    "        feats = torch.cat(feats)\n",
    "        feats = feats.cuda()\n",
    "        feat  = F.normalize(feat) \n",
    "        save_pickle(feats,f'{mode}_embedding.pkl')\n",
    "    del feat, img, dataset, dataloader\n",
    "    gc.collect()\n",
    "    if return_probs:\n",
    "        return df['id'].values, feats, prob_m\n",
    "    return df['id'].values,feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b77468",
   "metadata": {
    "papermill": {
     "duration": 0.034157,
     "end_time": "2021-09-29T20:03:34.075513",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.041356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LoFTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56a5b85d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:34.154156Z",
     "iopub.status.busy": "2021-09-29T20:03:34.153023Z",
     "iopub.status.idle": "2021-09-29T20:03:34.168345Z",
     "shell.execute_reply": "2021-09-29T20:03:34.169199Z",
     "shell.execute_reply.started": "2021-09-29T18:19:15.019173Z"
    },
    "papermill": {
     "duration": 0.059242,
     "end_time": "2021-09-29T20:03:34.169359",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.110117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_loftr(matcher, image_0, train_images):\n",
    "    img0 = []\n",
    "    img1 = []\n",
    "    img0_scales = []\n",
    "    img1_scales = []\n",
    "    for image_1 in train_images:\n",
    "        img0_raw = cv2.imread(image_0, cv2.IMREAD_GRAYSCALE)\n",
    "        img1_raw = cv2.imread(image_1, cv2.IMREAD_GRAYSCALE)\n",
    "        img0_scales.append((img0_raw.shape[1]/640, img0_raw.shape[0]/480))\n",
    "        img1_scales.append((img1_raw.shape[1]/640, img1_raw.shape[0]/480))\n",
    "        img0.append(cv2.resize(img0_raw, (640, 480)))\n",
    "        img1.append(cv2.resize(img1_raw, (640, 480)))\n",
    "        \n",
    "    del img0_raw, img1_raw\n",
    "        \n",
    "    img0 = torch.from_numpy(np.array(img0)[:,None,...]).cuda() / 255.\n",
    "    img1 = torch.from_numpy(np.array(img1)[:,None,...]).cuda() / 255.\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "    del img0 , img1\n",
    "    with torch.no_grad():\n",
    "        matcher(batch)\n",
    "#         mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "#         mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "#         mconf = batch['mconf'].cpu().numpy()\n",
    "    del batch['image0'], batch['image1']\n",
    "    batch['img0_scales'] = img0_scales\n",
    "    batch['img1_scales'] = img1_scales\n",
    "    gc.collect()\n",
    "    del matcher\n",
    "    return batch\n",
    "    \n",
    "def rescore_and_rerank(test_image_dir, train_image_dir,\n",
    "                                      test_image_id, train_ids_labels_and_scores, batch_size = 16, ignore_global_score=False, do_sort=True,\n",
    "                                      loftr_model=None, return_num_inliers=False):\n",
    "    \n",
    "    test_image_path = f'{test_image_dir}/{test_image_id[0]}/{test_image_id[1]}/{test_image_id[2]}/{test_image_id}.jpg'\n",
    "    test_image_dict = {}\n",
    "\n",
    "    ransac_inliers = []\n",
    "    \n",
    "    train_images_path = []\n",
    "    \n",
    "    for i in range(len(train_ids_labels_and_scores)):\n",
    "        train_image_id, label, global_score = train_ids_labels_and_scores[i]\n",
    "        train_images_path.append(f'{train_image_dir}/{train_image_id[0]}/{train_image_id[1]}/{train_image_id[2]}/{train_image_id}.jpg')\n",
    "        \n",
    "    for batch_i in range(0,len(train_images_path), batch_size):\n",
    "        pred = generate_loftr(loftr_model,test_image_path,train_images_path[batch_i:batch_i+batch_size] )\n",
    "        test_scales = pred['img0_scales'][0]\n",
    "        test_keypoints = copy.deepcopy(pred['mkpts0_f']).cpu().numpy()*test_scales[0]\n",
    "        m_bids = pred['m_bids'].cpu().numpy()\n",
    "        del test_scales\n",
    "        up = batch_size if batch_i + batch_size < len(train_images_path) else len(train_images_path) - batch_i\n",
    "        for i in range(up):\n",
    "            train_image_id, label, global_score = train_ids_labels_and_scores[batch_i + i]\n",
    "            train_scales = pred['img1_scales'][i]\n",
    "            train_keypoints = copy.deepcopy(pred['mkpts1_f'][m_bids == i]).cpu().numpy()*train_scales\n",
    "            num_inliers = compute_num_inliers(test_keypoints[m_bids == i], None, train_keypoints, None, do_kdtree=False)\n",
    "\n",
    "            total_score = global_score + num_inliers/MAX_INLIER_SCORE\n",
    "            train_ids_labels_and_scores[batch_i + i] = (train_image_id, label, total_score)\n",
    "            ransac_inliers.append((train_image_id, num_inliers))\n",
    "        del pred, test_keypoints, train_keypoints, train_scales\n",
    "        \n",
    "    if do_sort:\n",
    "        train_ids_labels_and_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "    gc.collect()   \n",
    "    if return_num_inliers:\n",
    "        return ransac_inliers\n",
    "    else:\n",
    "        return train_ids_labels_and_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ce4d2",
   "metadata": {
    "papermill": {
     "duration": 0.036383,
     "end_time": "2021-09-29T20:03:34.241220",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.204837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e74815b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:34.328959Z",
     "iopub.status.busy": "2021-09-29T20:03:34.327796Z",
     "iopub.status.idle": "2021-09-29T20:03:34.330396Z",
     "shell.execute_reply": "2021-09-29T20:03:34.330982Z",
     "shell.execute_reply.started": "2021-09-29T18:19:15.039619Z"
    },
    "papermill": {
     "duration": 0.054753,
     "end_time": "2021-09-29T20:03:34.331170",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.276417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_retrieval(labelmap, train_ids, train_embeddings,\n",
    "                 test_embeddings, num_to_rerank, val_x = None, val_y = None, do_dba=False,\n",
    "                 gallery_set='index', method = 'cossim',return_vals = False, name ='simple'):\n",
    "    train_ids_labels_and_scores = [None] * test_embeddings.shape[0]\n",
    "    \n",
    "    if method == 'faiss':\n",
    "        if do_dba:\n",
    "            faiss_index = faiss.IndexFlatIP(train_embeddings.shape[1])\n",
    "            faiss_index.add(train_embeddings)\n",
    "            dba_lens = 10\n",
    "            weights = np.logspace(0, -1.5, dba_lens)\n",
    "            weights /= np.sum(weights)\n",
    "            D, I = faiss_index.search(train_embeddings, dba_lens)\n",
    "            new_xb = 0\n",
    "            for i, weight in enumerate(weights):\n",
    "                new_xb = new_xb + train_embeddings[I[:, i]] * weight\n",
    "            train_embeddings = new_xb\n",
    "\n",
    "        faiss_index = faiss.IndexFlatIP(train_embeddings.shape[1])\n",
    "        faiss_index.add(train_embeddings)\n",
    "        D, I = faiss_index.search(test_embeddings, num_to_rerank)  # actual search\n",
    "    elif method == 'cossim':\n",
    "        if val_x is None:\n",
    "            D, I = get_topk_cossim(test_embeddings, train_embeddings, batchsize = 64, k=num_to_rerank, device='cuda:0',verbose=True, name = name)\n",
    "        else:\n",
    "            D, I = get_topk_cossim_sub(test_embeddings, train_embeddings,val_x, batchsize = 64, k=num_to_rerank, device='cuda:0',verbose=True, name = name)\n",
    "        \n",
    "        if val_y is not None:\n",
    "#             D -= 1* val_y\n",
    "            pass\n",
    "    \n",
    "    for test_index in range(test_embeddings.shape[0]):\n",
    "        train_ids_labels_and_scores[test_index] = [\n",
    "          (train_ids[train_index], labelmap[train_ids[train_index]], distance)\n",
    "          for train_index, distance in zip(I[test_index], D[test_index])\n",
    "        ]\n",
    "    del I\n",
    "    gc.collect()\n",
    "    if return_vals:\n",
    "        return train_ids_labels_and_scores, D\n",
    "    del D\n",
    "    return train_ids_labels_and_scores\n",
    "\n",
    "def get_nolandmark_by_dbscan(test_ids, test_embeddings, nolandmark_ids, nolandmark_embeddings):\n",
    "  # dbscan\n",
    "    features = np.vstack([test_embeddings, nolandmark_embeddings])\n",
    "    clusters = dbscan(eps=0.85, n_jobs=-1, min_samples=1).fit_predict(features)\n",
    "    clusters_np = np.c_[np.r_[test_ids, nolandmark_ids], clusters]\n",
    "    clusters_df = pd.DataFrame(data=clusters_np, columns=[ID, 'clusters'])\n",
    "    clusters_df['is_nolandmark'] = [0]*len(test_ids) + [1]*len(nolandmark_ids)\n",
    "    clusters_gb = clusters_df.groupby('clusters')['is_nolandmark'].agg(['count', 'sum']).reset_index()\n",
    "    clusters_gb.columns = ['clusters', 'clusters_num', 'nolandmark_num']\n",
    "    clusters_gb['nolandmark_rate'] = clusters_gb['nolandmark_num'] / clusters_gb['clusters_num']\n",
    "\n",
    "    test_clusters = clusters_df[0: len(test_ids)]\n",
    "    test_clusters = test_clusters.merge(clusters_gb, on='clusters', how='left')\n",
    "    gc.collect()\n",
    "    return test_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf38525",
   "metadata": {
    "papermill": {
     "duration": 0.033954,
     "end_time": "2021-09-29T20:03:34.400833",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.366879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CosSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5723896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:34.484703Z",
     "iopub.status.busy": "2021-09-29T20:03:34.477442Z",
     "iopub.status.idle": "2021-09-29T20:03:34.487748Z",
     "shell.execute_reply": "2021-09-29T20:03:34.487192Z",
     "shell.execute_reply.started": "2021-09-29T18:19:15.058278Z"
    },
    "papermill": {
     "duration": 0.053488,
     "end_time": "2021-09-29T20:03:34.487874",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.434386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cos_similarity_matrix(a, b):\n",
    "    sim_mt = torch.mm(a, b.transpose(0, 1))\n",
    "    return sim_mt\n",
    "\n",
    "def get_topk_cossim(test_emb, tr_emb, batchsize = 64, k=10, device='cuda:0',verbose=True, name = 'simple'):\n",
    "    tr_emb = torch.tensor(tr_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    test_emb = torch.tensor(test_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for test_batch in tqdm(test_emb.split(batchsize),disable=1-verbose):\n",
    "        sim_mat = cos_similarity_matrix(test_batch, tr_emb)\n",
    "        vals_batch, inds_batch = torch.topk(sim_mat, k=k, dim=1)\n",
    "        vals += [vals_batch.detach().cpu()]\n",
    "        inds += [inds_batch.detach().cpu()]\n",
    "    vals = torch.cat(vals)\n",
    "    inds = torch.cat(inds)\n",
    "    save_pickle([vals, inds],name)\n",
    "    gc.collect()\n",
    "    return vals, inds\n",
    "\n",
    "def get_topk_cossim_sub(test_emb, tr_emb, vals_x, batchsize = 64, k=10, device='cuda:0',verbose=True, name = 'simple'):\n",
    "    tr_emb = torch.tensor(tr_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    test_emb = torch.tensor(test_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    vals_x = torch.tensor(vals_x, dtype = torch.float32, device=torch.device(device))\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for test_batch in tqdm(test_emb.split(batchsize),disable=1-verbose):\n",
    "        sim_mat = cos_similarity_matrix(test_batch, tr_emb)\n",
    "        sim_mat = torch.clamp(sim_mat,0,1) - vals_x.repeat(sim_mat.shape[0], 1)\n",
    "        \n",
    "        vals_batch, inds_batch = torch.topk(sim_mat, k=k, dim=1)\n",
    "        vals += [vals_batch.detach().cpu()]\n",
    "        inds += [inds_batch.detach().cpu()]\n",
    "    vals = torch.cat(vals)\n",
    "    inds = torch.cat(inds)\n",
    "    save_pickle([vals, inds],name)\n",
    "    gc.collect()\n",
    "    return vals, inds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00e055",
   "metadata": {
    "papermill": {
     "duration": 0.033662,
     "end_time": "2021-09-29T20:03:34.555565",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.521903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef7be6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:34.635066Z",
     "iopub.status.busy": "2021-09-29T20:03:34.627592Z",
     "iopub.status.idle": "2021-09-29T20:03:34.638271Z",
     "shell.execute_reply": "2021-09-29T20:03:34.637719Z",
     "shell.execute_reply.started": "2021-09-29T18:19:15.074622Z"
    },
    "papermill": {
     "duration": 0.049006,
     "end_time": "2021-09-29T20:03:34.638400",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.589394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pickle(data, filename = 'tmp.pkl'):\n",
    "    print('\\tSaving {} to {}'.format(filename.split('.')[0], filename))\n",
    "    pickle.dump(data, open(os.path.join(hyperparameters.CACHE_DIR,filename), 'wb'))\n",
    "    del data\n",
    "    \n",
    "def load_pickle(filename):\n",
    "    print('\\tLoading {} from {}'.format(filename.split('.')[0], filename))\n",
    "    return pickle.load(open(os.path.join(hyperparameters.CACHE_DIR,filename),'rb'))\n",
    "\n",
    "def index_df():\n",
    "    image_ids = []\n",
    "    for i in glob.glob('../input/landmark-retrieval-2021/index/*/*/*/*.jpg'):\n",
    "        image_ids.append(i.split('/')[-1].split('.')[0])\n",
    "    df = pd.DataFrame({'id':image_ids, 'landmark_id': [-2]*len(image_ids)})\n",
    "    del image_ids\n",
    "    return df\n",
    "\n",
    "def targets_prob():\n",
    "    p = np.load('../input/glr-files/classes.npy')\n",
    "    idx2landmark_id = {ix:x for ix, x in enumerate(p)}\n",
    "    landmark_id2idx = {x:ix for ix, x in enumerate(p)}\n",
    "    del p\n",
    "\n",
    "    pred_mask = pd.Series(df_train.landmark_id.unique()).map(landmark_id2idx).values\n",
    "    gc.collect()\n",
    "    return pred_mask, idx2landmark_id, landmark_id2idx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a56d3f",
   "metadata": {
    "papermill": {
     "duration": 0.03459,
     "end_time": "2021-09-29T20:03:34.707150",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.672560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute inlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a941e91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:34.791550Z",
     "iopub.status.busy": "2021-09-29T20:03:34.790332Z",
     "iopub.status.idle": "2021-09-29T20:03:34.792836Z",
     "shell.execute_reply": "2021-09-29T20:03:34.793351Z",
     "shell.execute_reply.started": "2021-09-29T18:19:15.087329Z"
    },
    "papermill": {
     "duration": 0.050307,
     "end_time": "2021-09-29T20:03:34.793524",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.743217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_INLIER_SCORE = 70\n",
    "MAX_REPROJECTION_ERROR = 4.0\n",
    "MAX_RANSAC_ITERATIONS = 1000\n",
    "HOMOGRAPHY_CONFIDENCE = 0.99\n",
    "\n",
    "def compute_putative_matching_keypoints(test_keypoints,\n",
    "                                        test_descriptors,\n",
    "                                        train_keypoints,\n",
    "                                        train_descriptors,\n",
    "                                        max_distance=0.9):\n",
    "    \"\"\"Finds matches from `test_descriptors` to KD-tree of `train_descriptors`.\"\"\"\n",
    "\n",
    "    train_descriptor_tree = spatial.cKDTree(train_descriptors)\n",
    "    _, matches = train_descriptor_tree.query(\n",
    "      test_descriptors, distance_upper_bound=max_distance)\n",
    "\n",
    "    test_kp_count = test_keypoints.shape[0]\n",
    "    train_kp_count = train_keypoints.shape[0]\n",
    "\n",
    "    test_matching_keypoints = np.array([\n",
    "      test_keypoints[i,]\n",
    "      for i in range(test_kp_count)\n",
    "      if matches[i] != train_kp_count\n",
    "    ])\n",
    "    train_matching_keypoints = np.array([\n",
    "      train_keypoints[matches[i],]\n",
    "      for i in range(test_kp_count)\n",
    "      if matches[i] != train_kp_count\n",
    "    ])\n",
    "    gc.collect()\n",
    "    return test_matching_keypoints, train_matching_keypoints\n",
    "\n",
    "def compute_num_inliers(test_keypoints, test_descriptors, train_keypoints,\n",
    "                        train_descriptors, do_kdtree=True):\n",
    "    \"\"\"Returns the number of RANSAC inliers.\"\"\"\n",
    "\n",
    "    if do_kdtree:\n",
    "        test_match_kp, train_match_kp = compute_putative_matching_keypoints(\n",
    "            test_keypoints, test_descriptors, train_keypoints, train_descriptors)\n",
    "    else:\n",
    "        test_match_kp, train_match_kp = test_keypoints, train_keypoints\n",
    "    if test_match_kp.shape[0] <= 4:  # Min keypoints supported by `pydegensac.findHomography()`\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        _, mask = pydegensac.findHomography(test_match_kp, train_match_kp,\n",
    "                                        MAX_REPROJECTION_ERROR,\n",
    "                                        HOMOGRAPHY_CONFIDENCE,\n",
    "                                        MAX_RANSAC_ITERATIONS)\n",
    "    except np.linalg.LinAlgError:  # When det(H)=0, can't invert matrix.\n",
    "        return 0\n",
    "    gc.collect()\n",
    "    return int(copy.deepcopy(mask).astype(np.float32).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cce42",
   "metadata": {
    "papermill": {
     "duration": 0.03498,
     "end_time": "2021-09-29T20:03:34.863295",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.828315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e200625",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:34.948838Z",
     "iopub.status.busy": "2021-09-29T20:03:34.940819Z",
     "iopub.status.idle": "2021-09-29T20:03:46.954276Z",
     "shell.execute_reply": "2021-09-29T20:03:46.954836Z",
     "shell.execute_reply.started": "2021-09-29T18:19:15.100967Z"
    },
    "papermill": {
     "duration": 12.056958,
     "end_time": "2021-09-29T20:03:46.955021",
     "exception": false,
     "start_time": "2021-09-29T20:03:34.898063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ../input/google-mark-no1/epoch_2_score_0.0016468191840280442.pth\n"
     ]
    }
   ],
   "source": [
    "def load_model(model, model_file):\n",
    "    state_dict = torch.load(model_file)\n",
    "    if \"model_state_dict\" in state_dict.keys():\n",
    "        state_dict = state_dict[\"model_state_dict\"]\n",
    "    state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n",
    "#     del state_dict['final.weight']\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(f\"loaded {model_file}\")\n",
    "    del state_dict\n",
    "    model.eval()  \n",
    "    gc.collect()\n",
    "    return model\n",
    "\n",
    "model1 = LandmarkNet(81313, backbone = 'tf_efficientnet_b3_ns', pool_type = 'gridgem', metric = 'curricular', pretrained = False)\n",
    "model1 = load_model(model1,'../input/google-mark-no1/epoch_2_score_0.0016468191840280442.pth').cuda()\n",
    "# model2 = LandmarkNet(81313, backbone = 'tf_efficientnet_b6_ns', pool_type = 'gridgem', metric = 'arcface')\n",
    "# model2 = load_model(model,'')\n",
    "\n",
    "# model1 = LandmarkNet(81313, backbone = 'tf_efficientnet_b3', pool_type = 'gem',sub_center = True, pretrained = False)\n",
    "# model1 = load_model(model1,'../input/morizin-exp1/epoch_10_score_0.001845540972934233.pth').cuda()\n",
    "# model2 = LandmarkNet(81313, backbone = 'tf_efficientnet_b3', pool_type = 'gem',sub_center = True, pretrained = False)\n",
    "# model2 = load_model(model2,'../input/morizin-exp1/epoch_9_score_0.0015537621993018456.pth').cuda()\n",
    "models = [model1]\n",
    "del model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97807fc7",
   "metadata": {
    "papermill": {
     "duration": 0.03364,
     "end_time": "2021-09-29T20:03:47.023801",
     "exception": false,
     "start_time": "2021-09-29T20:03:46.990161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1e58d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:47.112045Z",
     "iopub.status.busy": "2021-09-29T20:03:47.109744Z",
     "iopub.status.idle": "2021-09-29T20:03:47.112820Z",
     "shell.execute_reply": "2021-09-29T20:03:47.113324Z",
     "shell.execute_reply.started": "2021-09-29T18:19:27.231073Z"
    },
    "papermill": {
     "duration": 0.055632,
     "end_time": "2021-09-29T20:03:47.113497",
     "exception": false,
     "start_time": "2021-09-29T20:03:47.057865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(mode = 'retrieval'):\n",
    "    if mode == 'retrieval':\n",
    "        train_image_dir = f'../input/landmark-retrieval-2021/index'\n",
    "        test_image_dir = f'../input/landmark-retrieval-2021/test'\n",
    "    elif mode == 'recognition':\n",
    "        train_image_dir = f'../input/landmark-recognition-2021/index'\n",
    "        test_image_dir = f'../input/landmark-recognition-2021/test'\n",
    "    \n",
    "    df_train = pd.read_csv(f'../input/landmark-{mode}-2021/train.csv')\n",
    "    df_test  = pd.read_csv(f'../input/landmark-{mode}-2021/sample_submission.csv')\n",
    "    df_nl    = pd.read_csv('../input/google-landmark-2021-validation/valid.csv')\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if mode == 'retrieval':\n",
    "        if len(df_test) == 1129:\n",
    "            shutil.copy('../input/landmark-retrieval-2021/sample_submission.csv','./submission.csv')\n",
    "            return './submission.csv'\n",
    "        \n",
    "        df_index = index_df()\n",
    "        train_ids, train_embeddings = extract_global_features(df_index, mode = 'index', prob_type = mode)\n",
    "        del df_index\n",
    "    else:\n",
    "        if len(df_test) == 10345:\n",
    "            shutil.copy('../input/landmark-recognition-2021/sample_submission.csv','./submission.csv')\n",
    "            return './submission.csv'\n",
    "        train_ids, train_embeddings = extract_global_features(df_train, mode = 'train', prob_type = mode)\n",
    "    \n",
    "    nolandmark_ids, nolandmark_embeddings = extract_global_features(df_nl, mode = 'nolandmark', prob_type = mode)\n",
    "    test_ids, test_embeddings = extract_global_features(df_test, mode = 'test', prob_type = mode)\n",
    "    \n",
    "    del df_train, df_test, df_nl\n",
    "    torch.cuda.empty_cache()\n",
    "    del globals()['models']\n",
    "    \n",
    "    if mode == 'retrieval':\n",
    "        labelmap = dict([(i, -2) for i in train_ids])\n",
    "    else:\n",
    "        labelmap = load_labelmap('../input/landmark-recognition-2021/train.csv')\n",
    "    nolandmark_labelmap = dict([(i, -1) for i in nolandmark_ids])\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    _, val_x = do_retrieval(nolandmark_labelmap, nolandmark_ids,\n",
    "                                                    nolandmark_embeddings, train_embeddings,\n",
    "                                                    hyperparameters.num_to_rerank, gallery_set='nolandmark',return_vals = True, name = 'train_nl_sim.pkl')\n",
    "    \n",
    "    _, val_y = do_retrieval(nolandmark_labelmap, nolandmark_ids,\n",
    "                                                    nolandmark_embeddings, test_embeddings,\n",
    "                                                    hyperparameters.num_to_rerank, gallery_set='nolandmark',return_vals = True, name = 'test_nl_sim.pkl')\n",
    "    del nolandmark_embeddings, nolandmark_ids, nolandmark_labelmap\n",
    "    \n",
    "    train_ids_labels_and_scores = do_retrieval(labelmap, train_ids,\n",
    "                                               train_embeddings, test_embeddings,\n",
    "                                               hyperparameters.num_to_rerank,val_x = val_x[:,:].mean(axis=1).detach().cpu().numpy(),val_y = val_y, gallery_set='train', name= 'train_test_sim.pkl')\n",
    "    torch.cuda.empty_cache()\n",
    "    del train_embeddings, test_embeddings, val_y, val_x\n",
    "    gc.collect()\n",
    "    \n",
    "#     matcher = LoFTR(config=default_cfg)\n",
    "#     matcher.load_state_dict(torch.load(\"/kaggle/temp/loftr-repo/weights/outdoor_ds.ckpt\")['state_dict'])\n",
    "#     matcher = matcher.eval().cuda()\n",
    "    \n",
    "#     for test_index, test_id in tqdm(enumerate(test_ids), total=len(test_ids), desc='do LoFTR'):\n",
    "#         train_ids_labels_and_scores[test_index] = rescore_and_rerank(test_image_dir, train_image_dir, test_id,\n",
    "#                                                 train_ids_labels_and_scores[test_index], loftr_model=matcher)\n",
    "    torch.cuda.empty_cache()\n",
    "    submit_fname = save_submission_csv(f'/kaggle/input/landmark-{mode}-2021', test_ids = test_ids, predictions=train_ids_labels_and_scores, mode = mode)\n",
    "#     save_pickle(train_ids_labels_and_scores, 'predicition_v1.pkl')\n",
    "    del train_ids_labels_and_scores\n",
    "    gc.collect()\n",
    "    return submit_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3bbb823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:47.192018Z",
     "iopub.status.busy": "2021-09-29T20:03:47.191192Z",
     "iopub.status.idle": "2021-09-29T20:03:48.961939Z",
     "shell.execute_reply": "2021-09-29T20:03:48.961347Z"
    },
    "papermill": {
     "duration": 1.809539,
     "end_time": "2021-09-29T20:03:48.962111",
     "exception": false,
     "start_time": "2021-09-29T20:03:47.152572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_v1    = pd.read_csv(get_prediction(mode = 'retrieval'))\n",
    "# trf_model = GLRTRFModel(hyperparameters.embed_dim).eval().cuda()\n",
    "# gcn_model = GATPairClassifier(nfeat=N_FEATURE, nhid=16, dropout=0.0, nheads=16, pooling='first').eval().cuda()\n",
    "# lgb_model = load_pickle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24952b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T20:03:49.043563Z",
     "iopub.status.busy": "2021-09-29T20:03:49.042401Z",
     "iopub.status.idle": "2021-09-29T20:03:49.063206Z",
     "shell.execute_reply": "2021-09-29T20:03:49.063736Z"
    },
    "papermill": {
     "duration": 0.066072,
     "end_time": "2021-09-29T20:03:49.063912",
     "exception": false,
     "start_time": "2021-09-29T20:03:48.997840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00084cdf8f600d00</td>\n",
       "      <td>39ff080e3b9e37d9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00141b8a5a729084</td>\n",
       "      <td>d75e248790c371d4 a0a13eb5924b395c 49dac2cf6777...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0044d82ea7654ece</td>\n",
       "      <td>80f1aba556c3de4e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00d5b448fa93e1b8</td>\n",
       "      <td>2c6f6cbaa3f586c6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>012436be7f659057</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>ff06f084134f4df6</td>\n",
       "      <td>acd395de725c6ffa 0adc1ff5ed5df4b5 4b57b48ac2b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>ff4135c3071f7b36</td>\n",
       "      <td>216d1bea7a259232 067a42c02294ce2c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>ff8b519e7dfc5506</td>\n",
       "      <td>59ca927b6e0c8a7a 452a2125ea39a713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>ffb08958f4e67f61</td>\n",
       "      <td>b6b5c29be4dd342d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>ffb4bfd04f23dee7</td>\n",
       "      <td>15a18361cd04474a 903dc20092c78b11 487ec2e66189...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1129 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                             images\n",
       "0     00084cdf8f600d00                                   39ff080e3b9e37d9\n",
       "1     00141b8a5a729084  d75e248790c371d4 a0a13eb5924b395c 49dac2cf6777...\n",
       "2     0044d82ea7654ece                                   80f1aba556c3de4e\n",
       "3     00d5b448fa93e1b8                                   2c6f6cbaa3f586c6\n",
       "4     012436be7f659057                                                NaN\n",
       "...                ...                                                ...\n",
       "1124  ff06f084134f4df6  acd395de725c6ffa 0adc1ff5ed5df4b5 4b57b48ac2b7...\n",
       "1125  ff4135c3071f7b36                  216d1bea7a259232 067a42c02294ce2c\n",
       "1126  ff8b519e7dfc5506                  59ca927b6e0c8a7a 452a2125ea39a713\n",
       "1127  ffb08958f4e67f61                                   b6b5c29be4dd342d\n",
       "1128  ffb4bfd04f23dee7  15a18361cd04474a 903dc20092c78b11 487ec2e66189...\n",
       "\n",
       "[1129 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d91e4a",
   "metadata": {
    "papermill": {
     "duration": 0.0354,
     "end_time": "2021-09-29T20:03:49.135014",
     "exception": false,
     "start_time": "2021-09-29T20:03:49.099614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 100.834885,
   "end_time": "2021-09-29T20:03:50.743206",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-29T20:02:09.908321",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
