{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5903db4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-29T19:31:20.853304Z",
     "iopub.status.busy": "2021-09-29T19:31:20.852434Z",
     "iopub.status.idle": "2021-09-29T19:32:48.772898Z",
     "shell.execute_reply": "2021-09-29T19:32:48.772259Z",
     "shell.execute_reply.started": "2021-09-13T19:39:34.81846Z"
    },
    "papermill": {
     "duration": 88.024499,
     "end_time": "2021-09-29T19:32:48.773073",
     "exception": false,
     "start_time": "2021-09-29T19:31:20.748574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/landmark/deps/timm-0.4.12-py3-none-any.whl\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (0.8.2+cu110)\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (1.7.1+cu110)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.12) (3.7.4.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.12) (1.19.5)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.12) (8.2.0)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.4.12\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/landmark/deps/yacs-0.1.8-py3-none-any.whl\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from yacs==0.1.8) (5.4.1)\r\n",
      "yacs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/landmark-1024dim\r\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from cvcore==0.0.1) (1.7.1+cu110)\r\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.7/site-packages (from cvcore==0.0.1) (0.4.12)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm->cvcore==0.0.1) (0.8.2+cu110)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->cvcore==0.0.1) (3.7.4.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->cvcore==0.0.1) (1.19.5)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm->cvcore==0.0.1) (8.2.0)\r\n",
      "Building wheels for collected packages: cvcore\r\n",
      "  Building wheel for cvcore (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for cvcore: filename=cvcore-0.0.1-py3-none-any.whl size=53235 sha256=863465af02a71b0197e473f33323837db112cf5479bfd75760797512679c453d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e6/42/36/ee568a4ce37f2592ef073b7809e65271e9ca5a77302931db14\r\n",
      "Successfully built cvcore\r\n",
      "Installing collected packages: cvcore\r\n",
      "Successfully installed cvcore-0.0.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/landmark/deps/timm-0.4.12-py3-none-any.whl\n",
    "!pip install /kaggle/input/landmark/deps/yacs-0.1.8-py3-none-any.whl\n",
    "!pip install /kaggle/input/landmark-1024dim/\n",
    "\n",
    "!mkdir sample\n",
    "!mkdir sample/0\n",
    "!mkdir sample/0/0\n",
    "!mkdir sample/0/0/d\n",
    "!cp /kaggle/input/landmark-recognition-2021/train/0/0/d/* sample/0/0/d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3578581f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:48.811203Z",
     "iopub.status.busy": "2021-09-29T19:32:48.810376Z",
     "iopub.status.idle": "2021-09-29T19:32:55.717420Z",
     "shell.execute_reply": "2021-09-29T19:32:55.716908Z",
     "shell.execute_reply.started": "2021-09-13T19:41:27.849318Z"
    },
    "papermill": {
     "duration": 6.929075,
     "end_time": "2021-09-29T19:32:55.717555",
     "exception": false,
     "start_time": "2021-09-29T19:32:48.788480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# os.environ[\"LRU_CACHE_CAPACITY\"] = \"3\"\n",
    "import gc\n",
    "import operator\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "torch.set_grad_enabled(False)\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from albumentations import Compose, Resize, SmallestMaxSize, CenterCrop\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/input/landmark/\")\n",
    "import cvcore\n",
    "from cvcore.config import get_cfg\n",
    "from cvcore.modeling.models import build_cls_model\n",
    "from cvcore.data.albu_augment import ResizeLongestEdge\n",
    "from timm.data.loader import PrefetchLoader\n",
    "from timm.data import ToNumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42851f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:55.756717Z",
     "iopub.status.busy": "2021-09-29T19:32:55.755119Z",
     "iopub.status.idle": "2021-09-29T19:32:55.757336Z",
     "shell.execute_reply": "2021-09-29T19:32:55.757726Z",
     "shell.execute_reply.started": "2021-09-13T19:41:35.528085Z"
    },
    "papermill": {
     "duration": 0.026254,
     "end_time": "2021-09-29T19:32:55.757856",
     "exception": false,
     "start_time": "2021-09-29T19:32:55.731602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset parameters:\n",
    "INPUT_DIR = os.path.join('..', 'input')\n",
    "TRAIN_LABELMAP_PATH = os.path.join(INPUT_DIR, \"landmark-recognition-2021\", \"train.csv\")\n",
    "DATASET_DIR = os.path.join(INPUT_DIR, 'landmark-recognition-2021')\n",
    "TEST_IMAGE_DIR = os.path.join(DATASET_DIR, 'test')\n",
    "TRAIN_IMAGE_DIR = os.path.join(DATASET_DIR, 'train')\n",
    "\n",
    "# DEBUGGING PARAMS:\n",
    "NUM_PUBLIC_TRAIN_IMAGES = 1580470 # Used to detect if in session or re-run.\n",
    "MAX_NUM_EMBEDDINGS = -1  # Set to > 1 to subsample dataset while debugging.\n",
    "\n",
    "# Retrieval & re-ranking parameters:\n",
    "NUM_TO_RERANK = 5\n",
    "TOP_K = 3 # Number of retrieved images used to make prediction for a test image.\n",
    "NON_LANDMARK_TOP_K = 3\n",
    "\n",
    "# Model parameters\n",
    "CONFIG_FILES = [\n",
    "    \"../input/landmark-1024dim/configs/v2s.yaml\",\n",
    "    \"../input/landmark/configs/v2l.yaml\",\n",
    "    \"../input/landmark/configs/v2m.yaml\",\n",
    "    \"../input/landmark-1024dim/configs/v2m.yaml\",\n",
    "    \"../input/landmark-1024dim/configs/b4.yaml\",\n",
    "    \"../input/landmark-1024dim/configs/vit-hybrid.yaml\",\n",
    "    \"../input/landmark/configs/b3.yaml\",\n",
    "]\n",
    "\n",
    "CHECKPOINTS = [\n",
    "    \"../input/landmark-weights/v2s_gem_sz448.pth\",\n",
    "    \"../input/landmark-weights/v2l_gem_sz640.pth\",\n",
    "    \"../input/landmark-weights/v2m_gem_sz768.pth\",\n",
    "    \"../input/landmark-weights/v2m_gem_sz768_1024.pth\",\n",
    "    \"../input/landmark-weights/b4_gem_sz800_1024.pth\",\n",
    "    \"../input/landmark-weights/vit.pth\",\n",
    "    \"../input/landmark-weights/b3_optd.pth\",\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    \"embeddings1024dim/v2s_gem_sz448\",\n",
    "    \"landmark-embeddings/v2l_gem_sz640\",\n",
    "    \"landmark-embeddings/v2m_gem_sz768\",\n",
    "    \"embeddings1024dim/v2m_gem_sz768_1024\",\n",
    "    \"embeddings1024dim/b4_gem_sz800\",\n",
    "    \"landmark-embeddings/vit_hybrid\",\n",
    "    \"landmark-embeddings/b3_optd\",\n",
    "]\n",
    "\n",
    "MODEL_INPUT_SHAPES = [\n",
    "    (576, 576),\n",
    "    (800, 800),\n",
    "    (1024, 1024),\n",
    "    (1024, 1024),\n",
    "    (800, 1024),\n",
    "    (384, 384),\n",
    "    (512, 512),\n",
    "]\n",
    "\n",
    "PREPROCESSING = [\n",
    "    \"center_crop\",\n",
    "    \"center_crop\",\n",
    "    \"mosaic\",\n",
    "    \"mosaic\",\n",
    "    \"resize\",\n",
    "    \"mosaic\",\n",
    "    \"center_crop\",\n",
    "]\n",
    "\n",
    "MODEL_WEIGHTS = [0.9, 0.9, 1.2, 1.1, 1.0, 0.8, 0.8]\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638a532c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:55.801463Z",
     "iopub.status.busy": "2021-09-29T19:32:55.793062Z",
     "iopub.status.idle": "2021-09-29T19:32:55.803462Z",
     "shell.execute_reply": "2021-09-29T19:32:55.803881Z",
     "shell.execute_reply.started": "2021-09-13T19:49:41.562654Z"
    },
    "papermill": {
     "duration": 0.032544,
     "end_time": "2021-09-29T19:32:55.804016",
     "exception": false,
     "start_time": "2021-09-29T19:32:55.771472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_size, image_aug=\"mosaic\", interp=1):\n",
    "        super(InferenceDataset, self).__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.img_aug = image_aug\n",
    "        \n",
    "        if self.img_aug == \"mosaic\":\n",
    "            self.img_size = image_size[0]\n",
    "            self.aug = ResizeLongestEdge(max_size=self.img_size, interpolation=interp)\n",
    "        elif self.img_aug == \"center_crop\":\n",
    "            self.aug = Compose([SmallestMaxSize(image_size[0], interpolation=interp), \n",
    "                                CenterCrop(height=image_size[0], width=image_size[0])])\n",
    "        elif self.img_aug == \"resize\":\n",
    "            self.aug = Resize(height=image_size[0], width=image_size[1], interpolation=interp)\n",
    "        print(self.aug)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        img = self.aug(image=img)[\"image\"]\n",
    "        img = ToNumpy()(img)\n",
    "        \n",
    "        if self.img_aug == \"mosaic\":\n",
    "            out = np.zeros((3, self.img_size, self.img_size), dtype=np.uint8)\n",
    "            out[:, : img.shape[1], : img.shape[2]] = img\n",
    "\n",
    "            pad_x, pad_y = img.shape[1], img.shape[2]\n",
    "            while pad_x < self.img_size:\n",
    "                begin, end = pad_x, min(out.shape[1], pad_x + img.shape[1])\n",
    "                out[:, begin:end, : img.shape[2]] = img[:, : end - begin, :]\n",
    "                pad_x += img.shape[1]\n",
    "\n",
    "            while pad_y < self.img_size:\n",
    "                begin, end = pad_y, min(out.shape[2], pad_y + img.shape[2])\n",
    "                out[:, :, begin:end] = out[:, :, : end - begin]\n",
    "                pad_y += img.shape[2]\n",
    "            return out, idx\n",
    "        \n",
    "        return img, idx\n",
    "\n",
    "\n",
    "def to_int(hex_id):\n",
    "    return int(hex_id, 16)\n",
    "\n",
    "\n",
    "def to_hex(image_id):\n",
    "    return \"{0:0{1}x}\".format(image_id, 16)\n",
    "\n",
    "\n",
    "def get_image_path(data_dir, image_id):\n",
    "    name = to_hex(image_id)\n",
    "    return os.path.join(data_dir, name[0], name[1], name[2], \"{}.jpg\".format(name))\n",
    "\n",
    "\n",
    "def load_labelmap():\n",
    "    with open(TRAIN_LABELMAP_PATH, mode=\"r\") as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        labelmap = {row[\"id\"]: row[\"landmark_id\"] for row in csv_reader}\n",
    "    return labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab74b9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:55.839132Z",
     "iopub.status.busy": "2021-09-29T19:32:55.838374Z",
     "iopub.status.idle": "2021-09-29T19:32:55.840734Z",
     "shell.execute_reply": "2021-09-29T19:32:55.840230Z",
     "shell.execute_reply.started": "2021-09-13T19:49:41.584883Z"
    },
    "papermill": {
     "duration": 0.023157,
     "end_time": "2021-09-29T19:32:55.840848",
     "exception": false,
     "start_time": "2021-09-29T19:32:55.817691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_global_features(image_root_dir, model, \n",
    "                            image_size=1024, image_aug=\"mosaic\"):\n",
    "    image_paths = [x for x in pathlib.Path(image_root_dir).rglob(\"*.jpg\")]\n",
    "    num_embeddings = len(image_paths)\n",
    "    print(\"Num images: \", num_embeddings)\n",
    "\n",
    "    ids = [to_int(image_path.name.split(\".\")[0]) for image_path in image_paths]\n",
    "    dataset = InferenceDataset(image_paths, image_size, image_aug)        \n",
    "    dataloader = PrefetchLoader(\n",
    "        DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        ),\n",
    "        fp16=True,\n",
    "    )\n",
    "    embeddings = []\n",
    "    for images, idxs in tqdm(dataloader):\n",
    "        with autocast():\n",
    "            global_descriptors = model._forward_features(images)\n",
    "            global_descriptors = F.normalize(global_descriptors, dim=1)\n",
    "        embeddings.append(global_descriptors.cpu())    \n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "    return ids, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a77da94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:55.874149Z",
     "iopub.status.busy": "2021-09-29T19:32:55.873420Z",
     "iopub.status.idle": "2021-09-29T19:32:55.875385Z",
     "shell.execute_reply": "2021-09-29T19:32:55.875817Z",
     "shell.execute_reply.started": "2021-09-13T19:49:41.60462Z"
    },
    "papermill": {
     "duration": 0.021665,
     "end_time": "2021-09-29T19:32:55.875948",
     "exception": false,
     "start_time": "2021-09-29T19:32:55.854283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_mean_similarities(source_embeddings, target_embeddings, k):\n",
    "    mean_source_similarities = []\n",
    "    for embeddings in tqdm(torch.split(source_embeddings, 4)):\n",
    "        similarities = torch.matmul(\n",
    "            torch.cat([emb.unsqueeze(0) for emb in embeddings], dim=0),\n",
    "            target_embeddings.T,\n",
    "        )\n",
    "        similarities = torch.topk(similarities, k=k, dim=1)[0]\n",
    "        scores = similarities.mean(dim=1)\n",
    "        mean_source_similarities.append(scores.cpu())\n",
    "    mean_source_similarities = torch.cat(mean_source_similarities, dim=0)\n",
    "    return mean_source_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1378edf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:55.914594Z",
     "iopub.status.busy": "2021-09-29T19:32:55.913806Z",
     "iopub.status.idle": "2021-09-29T19:32:55.915756Z",
     "shell.execute_reply": "2021-09-29T19:32:55.916176Z",
     "shell.execute_reply.started": "2021-09-13T19:54:42.118596Z"
    },
    "papermill": {
     "duration": 0.025021,
     "end_time": "2021-09-29T19:32:55.916289",
     "exception": false,
     "start_time": "2021-09-29T19:32:55.891268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_ids_labels_and_scores(labelmap, train_ids, train_embeddings,\n",
    "                                    test_embeddings, nl_embeddings, ens_w=1.0):\n",
    "    # Calculate non-landmark score for each train image\n",
    "    train_nl_scores = calculate_mean_similarities(train_embeddings, nl_embeddings, k=NON_LANDMARK_TOP_K).numpy()\n",
    "    # Calculate non-landmark score for each test image\n",
    "    test_nl_scores = calculate_mean_similarities(test_embeddings, nl_embeddings, k=NON_LANDMARK_TOP_K).numpy()\n",
    "    #     train_embeddings = train_embeddings.cpu()\n",
    "    #     test_embeddings = test_embeddings.cpu()\n",
    "    # Recognition-by-retrieval\n",
    "    train_ids_labels_and_scores = [None] * test_embeddings.shape[0]\n",
    "    test_indexes = torch.arange(len(test_embeddings))\n",
    "    test_indexes = torch.split(test_indexes, 4)\n",
    "    test_embeddings = torch.split(test_embeddings, 4)\n",
    "\n",
    "    for indexes, embeddings in zip(tqdm(test_indexes), test_embeddings):\n",
    "        chunk_similarities = torch.matmul(\n",
    "            torch.cat([emb.unsqueeze(0) for emb in embeddings], dim=0),\n",
    "            train_embeddings.T,\n",
    "        )\n",
    "        for test_index, similarities in zip(indexes, chunk_similarities):\n",
    "            partition = torch.argsort(similarities, descending=True)[:NUM_TO_RERANK]\n",
    "            partition = partition.cpu().numpy()\n",
    "            similarities = similarities.cpu().numpy()\n",
    "            # subtract by non-landmark scores \n",
    "            nearest = [\n",
    "                (train_ids[p], 2 * similarities[p] - train_nl_scores[p] - test_nl_scores[test_index]) \n",
    "                for p in partition\n",
    "            ]\n",
    "            nearest.sort(key=lambda x: x[1], reverse=True)\n",
    "            # pick top_k for each model\n",
    "            train_ids_labels_and_scores[test_index] = [\n",
    "                (train_id, labelmap[to_hex(train_id)], cosine_sim * ens_w)\n",
    "                for train_id, cosine_sim in nearest[:TOP_K]\n",
    "            ]\n",
    "    return train_ids_labels_and_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f36d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:55.949155Z",
     "iopub.status.busy": "2021-09-29T19:32:55.948407Z",
     "iopub.status.idle": "2021-09-29T19:32:55.950744Z",
     "shell.execute_reply": "2021-09-29T19:32:55.950259Z",
     "shell.execute_reply.started": "2021-09-13T19:54:42.134031Z"
    },
    "papermill": {
     "duration": 0.021041,
     "end_time": "2021-09-29T19:32:55.950859",
     "exception": false,
     "start_time": "2021-09-29T19:32:55.929818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction_map(test_ids, train_ids_labels_and_scores):\n",
    "    \"\"\"Makes dict from test ids and ranked training ids, labels, scores.\"\"\"\n",
    "\n",
    "    prediction_map = dict()\n",
    "\n",
    "    for test_index, test_id in enumerate(test_ids):\n",
    "        hex_test_id = to_hex(test_id)\n",
    "\n",
    "        aggregate_scores = {}\n",
    "        for _, label, score in train_ids_labels_and_scores[test_index]:\n",
    "            aggregate_scores[label] = aggregate_scores.get(label, 0) + score\n",
    "\n",
    "        label, score = max(aggregate_scores.items(), key=operator.itemgetter(1))\n",
    "\n",
    "        prediction_map[hex_test_id] = {\"score\": score, \"class\": label}\n",
    "\n",
    "    return prediction_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "207eb1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:55.981152Z",
     "iopub.status.busy": "2021-09-29T19:32:55.980315Z",
     "iopub.status.idle": "2021-09-29T19:32:55.997590Z",
     "shell.execute_reply": "2021-09-29T19:32:55.997189Z",
     "shell.execute_reply.started": "2021-09-13T20:10:23.779912Z"
    },
    "papermill": {
     "duration": 0.033315,
     "end_time": "2021-09-29T19:32:55.997692",
     "exception": false,
     "start_time": "2021-09-29T19:32:55.964377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(labelmap, public=False):\n",
    "    \"\"\"\n",
    "    Gets predictions using embedding similarity and local re-ranking.\n",
    "    \"\"\"\n",
    "    num_test_images = len([x for x in pathlib.Path(TEST_IMAGE_DIR).rglob(\"*.jpg\")])\n",
    "    print(f\"Found {num_test_images} images in {TEST_IMAGE_DIR}.\")    \n",
    "    train_landmarks = np.unique([int(v) for v in labelmap.values()])\n",
    "    \n",
    "    train_ids_labels_and_scores = [[]] * num_test_images\n",
    "    for i, (exper, cfg_file, ckpt) in enumerate(zip(EXPERIMENTS, CONFIG_FILES, CHECKPOINTS)):\n",
    "        cfg = get_cfg()\n",
    "        cfg.MODEL.BACKBONE.PRETRAINED = \"None\"\n",
    "        cfg.merge_from_file(cfg_file)\n",
    "        model = build_cls_model(cfg)\n",
    "        del model.cls_head\n",
    "        gc.collect()\n",
    "        checkpoint = torch.load(ckpt, \"cpu\").pop(\n",
    "            \"model\"\n",
    "        )\n",
    "        try:\n",
    "            del checkpoint[\"cls_head.weight\"]\n",
    "            gc.collect()\n",
    "        except:\n",
    "            print(\"Cls_head.weight has already been removed from state dict\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "        del checkpoint\n",
    "        gc.collect()\n",
    "        model.eval()\n",
    "        model = model.cuda()\n",
    "\n",
    "        image_size = MODEL_INPUT_SHAPES[i]\n",
    "        image_aug = PREPROCESSING[i]\n",
    "        ens_w = MODEL_WEIGHTS[i]\n",
    "        print(f\"{exper} - {image_aug} - {image_size} - {ens_w}\")\n",
    "        \n",
    "        train_ids, train_embeddings = extract_global_features(TRAIN_IMAGE_DIR, model, image_size, image_aug)\n",
    "        test_ids, test_embeddings = extract_global_features(TEST_IMAGE_DIR, model, image_size, image_aug)\n",
    "        \n",
    "        # Load non-landmark embeddings\n",
    "        emb_dict = torch.load(os.path.join(INPUT_DIR, f\"{exper}_non_landmark.pth\"), \"cpu\")\n",
    "        nl_embeddings = torch.stack([emb for emb in emb_dict.values()], 0)\n",
    "        del emb_dict; gc.collect()\n",
    "        # Augment train embeddings\n",
    "        for p in range(1, 5):\n",
    "            extra = pd.read_csv(os.path.join(INPUT_DIR, f\"landmark/train_cvdf_p{p}.csv\"))\n",
    "            extra = extra[extra[\"landmark_id\"].isin(train_landmarks)].reset_index(drop=True)\n",
    "            if public:\n",
    "                extra = extra.head(1000)\n",
    "            if len(extra) > 0:\n",
    "                emb_dict = torch.load(os.path.join(\n",
    "                    INPUT_DIR, f\"{exper}_index_recognition_p{p}.pth\"), \"cpu\")\n",
    "                train_extra_embeddings = torch.stack([emb_dict.get(id) for id in extra[\"id\"].values])\n",
    "                train_embeddings = torch.cat([train_embeddings, train_extra_embeddings], dim=0)\n",
    "                del emb_dict, train_extra_embeddings\n",
    "                gc.collect()\n",
    "                # append train ids + labels\n",
    "                for id, landmark_id in zip(extra[\"id\"].values, extra[\"landmark_id\"].values):\n",
    "                    train_ids.append(to_int(id))\n",
    "                    labelmap[id] = str(landmark_id)\n",
    "        \n",
    "        print(f\"Train embeddings shape {train_embeddings.shape}\")\n",
    "        print(f\"Test embeddings shape {test_embeddings.shape}\")\n",
    "        train_embeddings = train_embeddings.cuda()\n",
    "        test_embeddings = test_embeddings.cuda()\n",
    "        nl_embeddings = nl_embeddings.cuda()\n",
    "    \n",
    "        model_predictions = get_train_ids_labels_and_scores(\n",
    "            labelmap,\n",
    "            train_ids,\n",
    "            train_embeddings,\n",
    "            test_embeddings,\n",
    "            nl_embeddings,\n",
    "            ens_w,\n",
    "        )\n",
    "        train_ids_labels_and_scores = [\n",
    "            train_ids_labels_and_scores[test_index] + model_predictions[test_index]\n",
    "            for test_index in range(test_embeddings.shape[0])\n",
    "        ]\n",
    "\n",
    "        del train_embeddings\n",
    "        del test_embeddings\n",
    "        del nl_embeddings\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    pre_verification_predictions = get_prediction_map(\n",
    "        test_ids, train_ids_labels_and_scores\n",
    "    )\n",
    "    post_verification_predictions = None\n",
    "\n",
    "    return pre_verification_predictions, post_verification_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dcc5ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:56.031183Z",
     "iopub.status.busy": "2021-09-29T19:32:56.030402Z",
     "iopub.status.idle": "2021-09-29T19:32:56.032415Z",
     "shell.execute_reply": "2021-09-29T19:32:56.032854Z"
    },
    "papermill": {
     "duration": 0.021732,
     "end_time": "2021-09-29T19:32:56.032970",
     "exception": false,
     "start_time": "2021-09-29T19:32:56.011238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_submission_csv(predictions=None):\n",
    "    \"\"\"Saves optional `predictions` as submission.csv.\n",
    "\n",
    "    The csv has columns {id, landmarks}. The landmarks column is a string\n",
    "    containing the label and score for the id, separated by a ws delimeter.\n",
    "\n",
    "    If `predictions` is `None` (default), submission.csv is copied from\n",
    "    sample_submission.csv in `IMAGE_DIR`.\n",
    "\n",
    "    Args:\n",
    "    predictions: Optional dict of image ids to dicts with keys {class, score}.\n",
    "    \"\"\"\n",
    "\n",
    "    if predictions is None:\n",
    "        # Dummy submission!\n",
    "        shutil.copyfile(\n",
    "            os.path.join(DATASET_DIR, \"sample_submission.csv\"), \"submission.csv\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    with open(\"submission.csv\", \"w\") as submission_csv:\n",
    "        csv_writer = csv.DictWriter(submission_csv, fieldnames=[\"id\", \"landmarks\"])\n",
    "        csv_writer.writeheader()\n",
    "        for image_id, prediction in predictions.items():\n",
    "            label = prediction[\"class\"]\n",
    "            score = prediction[\"score\"]\n",
    "            csv_writer.writerow({\"id\": image_id, \"landmarks\": f\"{label} {score}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4853de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:32:56.065489Z",
     "iopub.status.busy": "2021-09-29T19:32:56.064997Z",
     "iopub.status.idle": "2021-09-29T19:54:10.509358Z",
     "shell.execute_reply": "2021-09-29T19:54:10.508812Z",
     "shell.execute_reply.started": "2021-09-13T20:10:25.506802Z"
    },
    "papermill": {
     "duration": 1274.463681,
     "end_time": "2021-09-29T19:54:10.510231",
     "exception": false,
     "start_time": "2021-09-29T19:32:56.046550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1580470 images in ../input/landmark-recognition-2021/train.\n",
      "Label map length 1580470.\n",
      "Found 344 images in sample.\n",
      "Cls_head.weight has already been removed from state dict\n",
      "embeddings1024dim/v2s_gem_sz448 - center_crop - (576, 576) - 0.9\n",
      "Num images:  344\n",
      "Compose([\n",
      "  SmallestMaxSize(always_apply=False, p=1, max_size=576, interpolation=1),\n",
      "  CenterCrop(always_apply=False, p=1.0, height=576, width=576),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:05<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num images:  344\n",
      "Compose([\n",
      "  SmallestMaxSize(always_apply=False, p=1, max_size=576, interpolation=1),\n",
      "  CenterCrop(always_apply=False, p=1.0, height=576, width=576),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:05<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape torch.Size([4344, 1024])\n",
      "Test embeddings shape torch.Size([344, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:02<00:00, 444.95it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 450.72it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 755.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cls_head.weight has already been removed from state dict\n",
      "landmark-embeddings/v2l_gem_sz640 - center_crop - (800, 800) - 0.9\n",
      "Num images:  344\n",
      "Compose([\n",
      "  SmallestMaxSize(always_apply=False, p=1, max_size=800, interpolation=1),\n",
      "  CenterCrop(always_apply=False, p=1.0, height=800, width=800),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:25<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num images:  344\n",
      "Compose([\n",
      "  SmallestMaxSize(always_apply=False, p=1, max_size=800, interpolation=1),\n",
      "  CenterCrop(always_apply=False, p=1.0, height=800, width=800),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:25<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape torch.Size([4344, 512])\n",
      "Test embeddings shape torch.Size([344, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:01<00:00, 574.21it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 577.47it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 772.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cls_head.weight has already been removed from state dict\n",
      "landmark-embeddings/v2m_gem_sz768 - mosaic - (1024, 1024) - 1.2\n",
      "Num images:  344\n",
      "ResizeLongestEdge(always_apply=False, p=1, max_size=1024, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:20<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num images:  344\n",
      "ResizeLongestEdge(always_apply=False, p=1, max_size=1024, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:20<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape torch.Size([4344, 512])\n",
      "Test embeddings shape torch.Size([344, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:01<00:00, 567.95it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 545.52it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 541.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cls_head.weight has already been removed from state dict\n",
      "embeddings1024dim/v2m_gem_sz768_1024 - mosaic - (1024, 1024) - 1.1\n",
      "Num images:  344\n",
      "ResizeLongestEdge(always_apply=False, p=1, max_size=1024, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:20<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num images:  344\n",
      "ResizeLongestEdge(always_apply=False, p=1, max_size=1024, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:20<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape torch.Size([4344, 1024])\n",
      "Test embeddings shape torch.Size([344, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:02<00:00, 447.29it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 451.87it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 769.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- init  32.0 0.3\n",
      "Cls_head.weight has already been removed from state dict\n",
      "embeddings1024dim/b4_gem_sz800 - resize - (800, 1024) - 1.0\n",
      "Num images:  344\n",
      "Resize(always_apply=False, p=1, height=800, width=1024, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:12<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num images:  344\n",
      "Resize(always_apply=False, p=1, height=800, width=1024, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:12<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape torch.Size([4344, 1024])\n",
      "Test embeddings shape torch.Size([344, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:02<00:00, 440.88it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 451.48it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 764.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized position embedding: torch.Size([1, 145, 1024]) to torch.Size([1, 145, 1024])\n",
      "Position embedding grid-size from 12 to 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cls_head.weight has already been removed from state dict\n",
      "landmark-embeddings/vit_hybrid - mosaic - (384, 384) - 0.8\n",
      "Num images:  344\n",
      "ResizeLongestEdge(always_apply=False, p=1, max_size=384, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:09<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num images:  344\n",
      "ResizeLongestEdge(always_apply=False, p=1, max_size=384, interpolation=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:09<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape torch.Size([4344, 512])\n",
      "Test embeddings shape torch.Size([344, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:01<00:00, 564.05it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 576.46it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 730.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark-embeddings/b3_optd - center_crop - (512, 512) - 0.8\n",
      "Num images:  344\n",
      "Compose([\n",
      "  SmallestMaxSize(always_apply=False, p=1, max_size=512, interpolation=1),\n",
      "  CenterCrop(always_apply=False, p=1.0, height=512, width=512),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:04<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num images:  344\n",
      "Compose([\n",
      "  SmallestMaxSize(always_apply=False, p=1, max_size=512, interpolation=1),\n",
      "  CenterCrop(always_apply=False, p=1.0, height=512, width=512),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:04<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape torch.Size([4344, 512])\n",
      "Test embeddings shape torch.Size([344, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1086/1086 [00:01<00:00, 552.86it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 568.71it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 761.02it/s]\n"
     ]
    }
   ],
   "source": [
    "num_training_images = len([x for x in pathlib.Path(TRAIN_IMAGE_DIR).rglob(\"*.jpg\")])\n",
    "print(f\"Found {num_training_images} images in {TRAIN_IMAGE_DIR}.\")\n",
    "\n",
    "if num_training_images == NUM_PUBLIC_TRAIN_IMAGES:\n",
    "    TEST_IMAGE_DIR = \"sample\"\n",
    "    TRAIN_IMAGE_DIR = \"sample\"\n",
    "\n",
    "labelmap = load_labelmap()\n",
    "print(f\"Label map length {len(labelmap)}.\")\n",
    "pre_verification_predictions, _ = get_predictions(labelmap, num_training_images == NUM_PUBLIC_TRAIN_IMAGES)\n",
    "\n",
    "if num_training_images == NUM_PUBLIC_TRAIN_IMAGES:\n",
    "    save_submission_csv()\n",
    "else:\n",
    "    save_submission_csv(pre_verification_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f460d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T19:54:10.972887Z",
     "iopub.status.busy": "2021-09-29T19:54:10.972179Z",
     "iopub.status.idle": "2021-09-29T19:54:10.975338Z",
     "shell.execute_reply": "2021-09-29T19:54:10.975771Z",
     "shell.execute_reply.started": "2021-09-13T20:13:17.649193Z"
    },
    "papermill": {
     "duration": 0.237537,
     "end_time": "2021-09-29T19:54:10.975926",
     "exception": false,
     "start_time": "2021-09-29T19:54:10.738389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00de1a596359e203': {'score': 7.1298012137413025, 'class': '66220'}, '00d261ebb1177e81': {'score': 6.471692276000977, 'class': '201489'}, '00d2800ec659cf28': {'score': 12.654741218686103, 'class': '108924'}, '00d0985d1103b65d': {'score': 8.234698796272278, 'class': '50570'}, '00dc8276176bb9c2': {'score': 8.783128148317337, 'class': '143997'}, '00dbd7e1158b2e2a': {'score': 8.841028380393983, 'class': '178259'}, '00d9611055a42262': {'score': 8.256806552410128, 'class': '173844'}, '00d6a5cf7641408a': {'score': 9.399436068534852, 'class': '165247'}, '00de491abff181f1': {'score': 8.161014342308045, 'class': '10090'}, '00dacbc535f0af33': {'score': 7.6377955973148355, 'class': '43568'}, '00d8deb092e878df': {'score': 9.043802642822266, 'class': '53761'}, '00df9b81f3b4c6ca': {'score': 7.418048942089081, 'class': '38310'}, '00ddc89cdad85d91': {'score': 8.034869861602784, 'class': '578'}, '00d6f2b1a4f22b7d': {'score': 9.205837219953537, 'class': '160778'}, '00d6718ac0d0a34b': {'score': 9.81110958456993, 'class': '157511'}, '00d0edcb5cc61ce8': {'score': 9.093550705909728, 'class': '184358'}, '00dbbbf446334b19': {'score': 8.556199765205383, 'class': '157473'}, '00d48ba5e0410aef': {'score': 9.50351565182209, 'class': '155818'}, '00d95ef7fc56de1e': {'score': 11.765472292900087, 'class': '126319'}, '00d5a5e8772ccb1a': {'score': 8.892940855026245, 'class': '66275'}, '00d1fca145f4d350': {'score': 8.81671695113182, 'class': '146159'}, '00d63c07678b2a9b': {'score': 8.744600039720536, 'class': '100895'}, '00df58f99061b237': {'score': 8.053168439865113, 'class': '94130'}, '00defeac847aa5d8': {'score': 8.894032663106918, 'class': '92901'}, '00d2677128e26f7b': {'score': 8.600420027971268, 'class': '153520'}, '00dd0df0b83b52cb': {'score': 7.048198413848877, 'class': '39166'}, '00d42334c101f09c': {'score': 9.330736124515534, 'class': '20541'}, '00d5b7a2e9c54e91': {'score': 8.880792051553726, 'class': '93770'}, '00d57570d4299bb5': {'score': 9.612659913301467, 'class': '180773'}, '00de129dec2ecc0e': {'score': 8.832192051410676, 'class': '171827'}, '00def269bb3f8693': {'score': 8.319202315807342, 'class': '191381'}, '00d48d186d2aba94': {'score': 8.385564434528352, 'class': '146624'}}\n"
     ]
    }
   ],
   "source": [
    "def subsample(x, n=32):\n",
    "    out = dict()\n",
    "    for k in list(x.keys())[:n]:\n",
    "        out[k] = x[k]\n",
    "    return out\n",
    "\n",
    "print(subsample(pre_verification_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc171ba",
   "metadata": {
    "papermill": {
     "duration": 0.226029,
     "end_time": "2021-09-29T19:54:11.431503",
     "exception": false,
     "start_time": "2021-09-29T19:54:11.205474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1380.004908,
   "end_time": "2021-09-29T19:54:14.220124",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-29T19:31:14.215216",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
