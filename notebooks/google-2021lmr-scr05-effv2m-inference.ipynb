{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe24ecd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.018799,
     "end_time": "2021-09-29T21:26:29.335733",
     "exception": false,
     "start_time": "2021-09-29T21:26:29.316934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About\n",
    "\n",
    "This notebook is a fork of https://www.kaggle.com/hidehisaarai1213/glret21-efficientnetb0-baseline-inference , please upvote the work of @hidehisaarai1213 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ea35bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:26:29.378265Z",
     "iopub.status.busy": "2021-09-29T21:26:29.377506Z",
     "iopub.status.idle": "2021-09-29T21:26:29.379221Z",
     "shell.execute_reply": "2021-09-29T21:26:29.378736Z",
     "shell.execute_reply.started": "2021-09-16T04:45:35.549386Z"
    },
    "papermill": {
     "duration": 0.025879,
     "end_time": "2021-09-29T21:26:29.379359",
     "exception": false,
     "start_time": "2021-09-29T21:26:29.353480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U keras-efficientnet-v2\n",
    "#!pip install -U git+https://github.com/leondgarse/keras_efficientnet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0ce2ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:26:29.509288Z",
     "iopub.status.busy": "2021-09-29T21:26:29.493669Z",
     "iopub.status.idle": "2021-09-29T21:26:34.002559Z",
     "shell.execute_reply": "2021-09-29T21:26:34.001675Z"
    },
    "papermill": {
     "duration": 4.606151,
     "end_time": "2021-09-29T21:26:34.002697",
     "exception": false,
     "start_time": "2021-09-29T21:26:29.396546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a EfficientNetV2 Model as defined in:\n",
    "Mingxing Tan, Quoc V. Le. (2021).\n",
    "EfficientNetV2: Smaller Models and Faster Training\n",
    "arXiv preprint arXiv:2104.00298.\n",
    "\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Add,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    DepthwiseConv2D,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling2D,\n",
    "    Input,\n",
    "    PReLU,\n",
    "    Reshape,\n",
    "    Multiply,\n",
    ")\n",
    "\n",
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 0.001\n",
    "CONV_KERNEL_INITIALIZER = keras.initializers.VarianceScaling(scale=2.0, mode=\"fan_out\", distribution=\"truncated_normal\")\n",
    "# CONV_KERNEL_INITIALIZER = 'glorot_uniform'\n",
    "\n",
    "BLOCK_CONFIGS = {\n",
    "    \"b0\": {  # width 1.0, depth 1.0\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 48, 96, 112, 192],\n",
    "        \"depthes\": [1, 2, 2, 3, 5, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "    },\n",
    "    \"b1\": {  # width 1.0, depth 1.1\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 48, 96, 112, 192],\n",
    "        \"depthes\": [2, 3, 3, 4, 6, 9],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "    },\n",
    "    \"b2\": {  # width 1.1, depth 1.2\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1408,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 56, 104, 120, 208],\n",
    "        \"depthes\": [2, 3, 3, 4, 6, 10],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "    },\n",
    "    \"b3\": {  # width 1.2, depth 1.4\n",
    "        \"first_conv_filter\": 40,\n",
    "        \"output_conv_filter\": 1536,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 40, 56, 112, 136, 232],\n",
    "        \"depthes\": [2, 3, 3, 5, 7, 12],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "    },\n",
    "    \"t\": {  # width 1.4 * 0.8, depth 1.8 * 0.9, from timm\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1024,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 40, 48, 104, 128, 208],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 14],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "    },\n",
    "    \"s\": {  # width 1.4, depth 1.8\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 48, 64, 128, 160, 256],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 15],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "    },\n",
    "    \"m\": {  # width 1.6, depth 2.2\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [24, 48, 80, 160, 176, 304, 512],\n",
    "        \"depthes\": [3, 5, 5, 7, 14, 18, 5],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "    },\n",
    "    \"l\": {  # width 2.0, depth 3.1\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [32, 64, 96, 192, 224, 384, 640],\n",
    "        \"depthes\": [4, 7, 7, 10, 19, 25, 7],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "    },\n",
    "    \"xl\": {\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [32, 64, 96, 192, 256, 512, 640],\n",
    "        \"depthes\": [4, 8, 8, 16, 24, 32, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "    },\n",
    "}\n",
    "\n",
    "FILE_HASH_DICT = {\n",
    "    \"b0\": {\"21k-ft1k\": \"4e4da4eb629897e4d6271e131039fe75\", \"21k\": \"5dbb4252df24b931e74cdd94d150f25a\", \"imagenet\": \"9abdc43cb00f4cb06a8bdae881f412d6\"},\n",
    "    \"b1\": {\"21k-ft1k\": \"5f1aee82209f4f0f20bd24460270564e\", \"21k\": \"a50ae65b50ceff7f5283be2f4506d2c2\", \"imagenet\": \"5d4223b59ff268828d5112a1630e234e\"},\n",
    "    \"b2\": {\"21k-ft1k\": \"ec384b84441ddf6419938d1e5a0cbef2\", \"21k\": \"9f718a8bbb7b63c5313916c5e504790d\", \"imagenet\": \"1814bc08d4bb7a5e0ed3ccfe1cf18650\"},\n",
    "    \"b3\": {\"21k-ft1k\": \"4a27827b0b2df508bed31ae231003bb1\", \"21k\": \"ade5bdbbdf1d54c4561aa41511525855\", \"imagenet\": \"cda85b8494c7ec5a68dffb335a254bab\"},\n",
    "    \"l\": {\"21k-ft1k\": \"30327edcf1390d10e9a0de42a2d731e3\", \"21k\": \"7970f913eec1b4918e007c8580726412\", \"imagenet\": \"2b65f5789f4d2f1bf66ecd6d9c5c2d46\"},\n",
    "    \"m\": {\"21k-ft1k\": \"0c236c3020e3857de1e5f2939abd0cc6\", \"21k\": \"3923c286366b2a5137f39d1e5b14e202\", \"imagenet\": \"ac3fd0ff91b35d18d1df8f1895efe1d5\"},\n",
    "    \"s\": {\"21k-ft1k\": \"93046a0d601da46bfce9d4ca14224c83\", \"21k\": \"10b05d878b64f796ab984a5316a4a1c3\", \"imagenet\": \"3b91df2c50c7a56071cca428d53b8c0d\"},\n",
    "    \"t\": {\"imagenet\": \"46632458117102758518158bf35444d7\"},\n",
    "    \"xl\": {\"21k-ft1k\": \"9aaa2bd3c9495b23357bc6593eee5bce\", \"21k\": \"c97de2770f55701f788644336181e8ee\"},\n",
    "    \"v1-b0\": {\"noisy_student\": \"d125a518737c601f8595937219243432\", \"imagenet\": \"cc7d08887de9df8082da44ce40761986\"},\n",
    "    \"v1-b1\": {\"noisy_student\": \"8f44bff58fc5ef99baa3f163b3f5c5e8\", \"imagenet\": \"a967f7be55a0125c898d650502c0cfd0\"},\n",
    "    \"v1-b2\": {\"noisy_student\": \"b4ffed8b9262df4facc5e20557983ef8\", \"imagenet\": \"6c8d1d3699275c7d1867d08e219e00a7\"},\n",
    "    \"v1-b3\": {\"noisy_student\": \"9d696365378a1ebf987d0e46a9d26ddd\", \"imagenet\": \"d78edb3dc7007721eda781c04bd4af62\"},\n",
    "    \"v1-b4\": {\"noisy_student\": \"a0f61b977544493e6926186463d26294\", \"imagenet\": \"4c83aa5c86d58746a56675565d4f2051\"},\n",
    "    \"v1-b5\": {\"noisy_student\": \"c3b6eb3f1f7a1e9de6d9a93e474455b1\", \"imagenet\": \"0bda50943b8e8d0fadcbad82c17c40f5\"},\n",
    "    \"v1-b6\": {\"noisy_student\": \"20dd18b0df60cd7c0387c8af47bd96f8\", \"imagenet\": \"da13735af8209f675d7d7d03a54bfa27\"},\n",
    "    \"v1-b7\": {\"noisy_student\": \"7f6f6dd4e8105e32432607ad28cfad0f\", \"imagenet\": \"d9c22b5b030d1e4f4c3a96dbf5f21ce6\"},\n",
    "    \"v1-l2\": {\"noisy_student\": \"5fedc721febfca4b08b03d1f18a4a3ca\"},\n",
    "}\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor=4, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def conv2d_no_bias(inputs, filters, kernel_size, strides=1, padding=\"VALID\", name=\"\"):\n",
    "    return Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=False, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name + \"conv\")(\n",
    "        inputs\n",
    "    )\n",
    "\n",
    "\n",
    "def batchnorm_with_activation(inputs, activation=\"swish\", name=\"\"):\n",
    "    \"\"\"Performs a batch normalization followed by an activation. \"\"\"\n",
    "    bn_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    nn = BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=BATCH_NORM_EPSILON,\n",
    "        name=name + \"bn\",\n",
    "    )(inputs)\n",
    "    if activation:\n",
    "        nn = Activation(activation=activation, name=name + activation)(nn)\n",
    "        # nn = PReLU(shared_axes=[1, 2], alpha_initializer=tf.initializers.Constant(0.25), name=name + \"PReLU\")(nn)\n",
    "    return nn\n",
    "\n",
    "\n",
    "def se_module(inputs, se_ratio=4, name=\"\"):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    h_axis, w_axis = [2, 3] if K.image_data_format() == \"channels_first\" else [1, 2]\n",
    "\n",
    "    filters = inputs.shape[channel_axis]\n",
    "    # reduction = _make_divisible(filters // se_ratio, 8)\n",
    "    reduction = filters // se_ratio\n",
    "    # se = GlobalAveragePooling2D()(inputs)\n",
    "    # se = Reshape((1, 1, filters))(se)\n",
    "    se = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)\n",
    "    se = Conv2D(reduction, kernel_size=1, use_bias=True, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name + \"1_conv\")(se)\n",
    "    # se = PReLU(shared_axes=[1, 2])(se)\n",
    "    se = Activation(\"swish\")(se)\n",
    "    se = Conv2D(filters, kernel_size=1, use_bias=True, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name + \"2_conv\")(se)\n",
    "    se = Activation(\"sigmoid\")(se)\n",
    "    return Multiply()([inputs, se])\n",
    "\n",
    "\n",
    "def MBConv(inputs, output_channel, stride, expand_ratio, shortcut, kernel_size=3, drop_rate=0, use_se=0, is_fused=False, name=\"\"):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    input_channel = inputs.shape[channel_axis]\n",
    "\n",
    "    if is_fused and expand_ratio != 1:\n",
    "        nn = conv2d_no_bias(inputs, input_channel * expand_ratio, (3, 3), strides=stride, padding=\"same\", name=name + \"sortcut_\")\n",
    "        nn = batchnorm_with_activation(nn, name=name + \"sortcut_\")\n",
    "    elif expand_ratio != 1:\n",
    "        nn = conv2d_no_bias(inputs, input_channel * expand_ratio, (1, 1), strides=(1, 1), padding=\"same\", name=name + \"sortcut_\")\n",
    "        nn = batchnorm_with_activation(nn, name=name + \"sortcut_\")\n",
    "    else:\n",
    "        nn = inputs\n",
    "\n",
    "    if not is_fused:\n",
    "        # nn = keras.layers.ZeroPadding2D(padding=1, name=name + \"pad\")(nn)\n",
    "        nn = DepthwiseConv2D(kernel_size, padding=\"same\", strides=stride, use_bias=False, depthwise_initializer=CONV_KERNEL_INITIALIZER, name=name + \"MB_dw_\")(\n",
    "            nn\n",
    "        )\n",
    "        nn = batchnorm_with_activation(nn, name=name + \"MB_dw_\")\n",
    "\n",
    "    if use_se:\n",
    "        nn = se_module(nn, se_ratio=4 * expand_ratio, name=name + \"se_\")\n",
    "\n",
    "    # pw-linear\n",
    "    if is_fused and expand_ratio == 1:\n",
    "        nn = conv2d_no_bias(nn, output_channel, (3, 3), strides=stride, padding=\"same\", name=name + \"fu_\")\n",
    "        nn = batchnorm_with_activation(nn, name=name + \"fu_\")\n",
    "    else:\n",
    "        nn = conv2d_no_bias(nn, output_channel, (1, 1), strides=(1, 1), padding=\"same\", name=name + \"MB_pw_\")\n",
    "        nn = batchnorm_with_activation(nn, activation=None, name=name + \"MB_pw_\")\n",
    "\n",
    "    if shortcut:\n",
    "        if drop_rate > 0:\n",
    "            nn = Dropout(drop_rate, noise_shape=(None, 1, 1, 1), name=name + \"drop\")(nn)\n",
    "        return Add()([inputs, nn])\n",
    "    else:\n",
    "        return nn\n",
    "\n",
    "\n",
    "def EfficientNetV2(\n",
    "    model_type,\n",
    "    input_shape=(None, None, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    first_strides=2,\n",
    "    drop_connect_rate=0,\n",
    "    classifier_activation=\"softmax\",\n",
    "    include_preprocessing=False,\n",
    "    pretrained=\"imagenet\",\n",
    "    model_name=\"EfficientNetV2\",\n",
    "    kwargs=None,  # Not used, just recieving parameter\n",
    "):\n",
    "    if isinstance(model_type, dict):  # For EfficientNetV1 configures\n",
    "        model_type, blocks_config = model_type.popitem()\n",
    "    else:\n",
    "        blocks_config = BLOCK_CONFIGS.get(model_type.lower(), BLOCK_CONFIGS[\"s\"])\n",
    "    expands = blocks_config[\"expands\"]\n",
    "    out_channels = blocks_config[\"out_channels\"]\n",
    "    depthes = blocks_config[\"depthes\"]\n",
    "    strides = blocks_config[\"strides\"]\n",
    "    use_ses = blocks_config[\"use_ses\"]\n",
    "    first_conv_filter = blocks_config.get(\"first_conv_filter\", out_channels[0])\n",
    "    output_conv_filter = blocks_config.get(\"output_conv_filter\", 1280)\n",
    "    kernel_sizes = blocks_config.get(\"kernel_sizes\", [3] * len(depthes))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    if include_preprocessing:\n",
    "        channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "        try:\n",
    "            Rescaling = keras.layers.Rescaling\n",
    "            Normalization = keras.layers.Normalization\n",
    "        except:\n",
    "            Rescaling = keras.layers.experimental.preprocessing.Rescaling\n",
    "            Normalization = keras.layers.experimental.preprocessing.Normalization\n",
    "        nn = Rescaling(1.0 / 255.0)(inputs)\n",
    "        nn = Normalization(mean=[0.485, 0.456, 0.406], variance=[0.229, 0.224, 0.225], axis=channel_axis)(nn)\n",
    "    else:\n",
    "        nn = inputs\n",
    "    out_channel = _make_divisible(first_conv_filter, 8)\n",
    "    nn = conv2d_no_bias(nn, out_channel, (3, 3), strides=first_strides, padding=\"same\", name=\"stem_\")\n",
    "    nn = batchnorm_with_activation(nn, name=\"stem_\")\n",
    "\n",
    "    pre_out = out_channel\n",
    "    global_block_id = 0\n",
    "    total_blocks = sum(depthes)\n",
    "    for id, (expand, out_channel, depth, stride, se, kernel_size) in enumerate(zip(expands, out_channels, depthes, strides, use_ses, kernel_sizes)):\n",
    "        out = _make_divisible(out_channel, 8)\n",
    "        is_fused = True if se == 0 else False\n",
    "        for block_id in range(depth):\n",
    "            stride = stride if block_id == 0 else 1\n",
    "            shortcut = True if out == pre_out and stride == 1 else False\n",
    "            name = \"stack_{}_block{}_\".format(id, block_id)\n",
    "            block_drop_rate = drop_connect_rate * global_block_id / total_blocks\n",
    "            nn = MBConv(nn, out, stride, expand, shortcut, kernel_size, block_drop_rate, se, is_fused, name=name)\n",
    "            pre_out = out\n",
    "            global_block_id += 1\n",
    "\n",
    "    output_conv_filter = _make_divisible(output_conv_filter, 8)\n",
    "    nn = conv2d_no_bias(nn, output_conv_filter, (1, 1), strides=(1, 1), padding=\"valid\", name=\"post_\")\n",
    "    nn = batchnorm_with_activation(nn, name=\"post_\")\n",
    "\n",
    "    if num_classes > 0:\n",
    "        nn = GlobalAveragePooling2D(name=\"avg_pool\")(nn)\n",
    "        if dropout > 0 and dropout < 1:\n",
    "            nn = Dropout(dropout)(nn)\n",
    "        nn = Dense(num_classes, activation=classifier_activation, dtype=\"float32\", name=\"predictions\")(nn)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=nn, name=model_name)\n",
    "    reload_model_weights(model, model_type, pretrained)\n",
    "    return model\n",
    "\n",
    "\n",
    "def reload_model_weights(model, model_type, pretrained=\"imagenet\"):\n",
    "    pretrained_dd = {\"imagenet\": \"imagenet\", \"imagenet21k\": \"21k\", \"imagenet21k-ft1k\": \"21k-ft1k\", \"noisy_student\": \"noisy_student\"}\n",
    "    if not pretrained in pretrained_dd:\n",
    "        print(\">>>> No pretrained available, model will be randomly initialized\")\n",
    "        return\n",
    "    pre_tt = pretrained_dd[pretrained]\n",
    "    if model_type not in FILE_HASH_DICT or pre_tt not in FILE_HASH_DICT[model_type]:\n",
    "        print(\">>>> No pretrained available, model will be randomly initialized\")\n",
    "        return\n",
    "\n",
    "    if model_type.startswith(\"v1\"):\n",
    "        pre_url = \"https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv1_pretrained/efficientnet{}-{}.h5\"\n",
    "    else:\n",
    "        pre_url = \"https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv2_pretrained/efficientnetv2-{}-{}.h5\"\n",
    "    url = pre_url.format(model_type, pre_tt)\n",
    "    file_name = os.path.basename(url)\n",
    "    file_hash = FILE_HASH_DICT[model_type][pre_tt]\n",
    "\n",
    "    try:\n",
    "        pretrained_model = keras.utils.get_file(file_name, url, cache_subdir=\"models/efficientnetv2\", file_hash=file_hash)\n",
    "    except:\n",
    "        print(\"[Error] will not load weights, url not found or download failed:\", url)\n",
    "        return\n",
    "    else:\n",
    "        print(\">>>> Load pretrained from:\", pretrained_model)\n",
    "        model.load_weights(pretrained_model, by_name=True, skip_mismatch=True)\n",
    "\n",
    "\n",
    "def EfficientNetV2B0(input_shape=(224, 224, 3), num_classes=1000, dropout=0.2, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"b0\", model_name=\"EfficientNetV2B0\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2B1(input_shape=(240, 240, 3), num_classes=1000, dropout=0.2, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"b1\", model_name=\"EfficientNetV2B1\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2B2(input_shape=(260, 260, 3), num_classes=1000, dropout=0.3, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"b2\", model_name=\"EfficientNetV2B2\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2B3(input_shape=(300, 300, 3), num_classes=1000, dropout=0.3, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"b3\", model_name=\"EfficientNetV2B3\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2T(input_shape=(320, 320, 3), num_classes=1000, dropout=0.2, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"t\", model_name=\"EfficientNetV2T\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2S(input_shape=(384, 384, 3), num_classes=1000, dropout=0.2, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"s\", model_name=\"EfficientNetV2S\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2M(input_shape=(480, 480, 3), num_classes=1000, dropout=0.3, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"m\", model_name=\"EfficientNetV2M\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2L(input_shape=(480, 480, 3), num_classes=1000, dropout=0.4, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"l\", model_name=\"EfficientNetV2L\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetV2XL(input_shape=(512, 512, 3), num_classes=1000, dropout=0.4, classifier_activation=\"softmax\", pretrained=\"imagenet21k-ft1k\", **kwargs):\n",
    "    return EfficientNetV2(model_type=\"xl\", model_name=\"EfficientNetV2XL\", **locals(), **kwargs)\n",
    "\n",
    "\n",
    "def get_actual_drop_connect_rates(model):\n",
    "    return [ii.rate for ii in model.layers if isinstance(ii, keras.layers.Dropout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5f8f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:26:34.044555Z",
     "iopub.status.busy": "2021-09-29T21:26:34.043719Z",
     "iopub.status.idle": "2021-09-29T21:27:30.944012Z",
     "shell.execute_reply": "2021-09-29T21:27:30.943098Z",
     "shell.execute_reply.started": "2021-08-31T13:35:02.611425Z"
    },
    "papermill": {
     "duration": 56.923704,
     "end_time": "2021-09-29T21:27:30.944156",
     "exception": false,
     "start_time": "2021-09-29T21:26:34.020452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/kerasapplications/ > /dev/null\n",
    "!pip install ../input/efficientnet-keras-source-code/ > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51439be6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:30.988908Z",
     "iopub.status.busy": "2021-09-29T21:27:30.987640Z",
     "iopub.status.idle": "2021-09-29T21:27:31.893379Z",
     "shell.execute_reply": "2021-09-29T21:27:31.892909Z",
     "shell.execute_reply.started": "2021-08-31T13:43:31.019104Z"
    },
    "papermill": {
     "duration": 0.930717,
     "end_time": "2021-09-29T21:27:31.893511",
     "exception": false,
     "start_time": "2021-09-29T21:27:30.962794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import efficientnet.tfkeras as efn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fa40d",
   "metadata": {
    "papermill": {
     "duration": 0.018661,
     "end_time": "2021-09-29T21:27:31.931214",
     "exception": false,
     "start_time": "2021-09-29T21:27:31.912553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b08e4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:31.972904Z",
     "iopub.status.busy": "2021-09-29T21:27:31.972371Z",
     "iopub.status.idle": "2021-09-29T21:27:31.975855Z",
     "shell.execute_reply": "2021-09-29T21:27:31.976227Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.230428Z"
    },
    "papermill": {
     "duration": 0.026608,
     "end_time": "2021-09-29T21:27:31.976390",
     "exception": false,
     "start_time": "2021-09-29T21:27:31.949782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/landmark-retrieval-2021/\")\n",
    "TEST_IMAGE_DIR = DATADIR / \"test\"\n",
    "TRAIN_IMAGE_DIR = DATADIR / \"index\"\n",
    "\n",
    "TOPK = 100\n",
    "N_CLASSES = 81313\n",
    "\n",
    "SEED = 20210912"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578466f5",
   "metadata": {
    "papermill": {
     "duration": 0.018328,
     "end_time": "2021-09-29T21:27:32.013874",
     "exception": false,
     "start_time": "2021-09-29T21:27:31.995546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a25be3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.057604Z",
     "iopub.status.busy": "2021-09-29T21:27:32.056802Z",
     "iopub.status.idle": "2021-09-29T21:27:32.059567Z",
     "shell.execute_reply": "2021-09-29T21:27:32.059139Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.236667Z"
    },
    "papermill": {
     "duration": 0.026648,
     "end_time": "2021-09-29T21:27:32.059680",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.033032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    print(f\"[{name}]\")\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316fcde0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.103007Z",
     "iopub.status.busy": "2021-09-29T21:27:32.102468Z",
     "iopub.status.idle": "2021-09-29T21:27:32.105937Z",
     "shell.execute_reply": "2021-09-29T21:27:32.106356Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.245696Z"
    },
    "papermill": {
     "duration": 0.027474,
     "end_time": "2021-09-29T21:27:32.106491",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.079017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955b1f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.149048Z",
     "iopub.status.busy": "2021-09-29T21:27:32.148333Z",
     "iopub.status.idle": "2021-09-29T21:27:32.151059Z",
     "shell.execute_reply": "2021-09-29T21:27:32.150662Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.254819Z"
    },
    "papermill": {
     "duration": 0.025888,
     "end_time": "2021-09-29T21:27:32.151164",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.125276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b31e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.191819Z",
     "iopub.status.busy": "2021-09-29T21:27:32.191295Z",
     "iopub.status.idle": "2021-09-29T21:27:32.201315Z",
     "shell.execute_reply": "2021-09-29T21:27:32.202423Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.262405Z"
    },
    "papermill": {
     "duration": 0.032867,
     "end_time": "2021-09-29T21:27:32.202587",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.169720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 1 replicas\n"
     ]
    }
   ],
   "source": [
    "strategy = auto_select_accelerator()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6e7b5",
   "metadata": {
    "papermill": {
     "duration": 0.018648,
     "end_time": "2021-09-29T21:27:32.240485",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.221837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92bb6600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.288586Z",
     "iopub.status.busy": "2021-09-29T21:27:32.287825Z",
     "iopub.status.idle": "2021-09-29T21:27:32.290452Z",
     "shell.execute_reply": "2021-09-29T21:27:32.289936Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.276517Z"
    },
    "papermill": {
     "duration": 0.031264,
     "end_time": "2021-09-29T21:27:32.290563",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.259299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeM(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size, init_norm=3.0, normalize=False, **kwargs):\n",
    "        self.pool_size = pool_size\n",
    "        self.init_norm = init_norm\n",
    "        self.normalize = normalize\n",
    "\n",
    "        super(GeM, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'pool_size': self.pool_size,\n",
    "            'init_norm': self.init_norm,\n",
    "            'normalize': self.normalize,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_size = input_shape[-1]\n",
    "        self.p = self.add_weight(name='norms', shape=(feature_size,),\n",
    "                                 initializer=tf.keras.initializers.constant(self.init_norm),\n",
    "                                 trainable=True)\n",
    "        super(GeM, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = tf.math.maximum(x, 1e-6)\n",
    "        x = tf.pow(x, self.p)\n",
    "\n",
    "        x = tf.nn.avg_pool(x, self.pool_size, self.pool_size, 'VALID')\n",
    "        x = tf.pow(x, 1.0 / self.p)\n",
    "\n",
    "        if self.normalize:\n",
    "            x = tf.nn.l2_normalize(x, 1)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, input_shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41bfe306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.342046Z",
     "iopub.status.busy": "2021-09-29T21:27:32.336015Z",
     "iopub.status.idle": "2021-09-29T21:27:32.344635Z",
     "shell.execute_reply": "2021-09-29T21:27:32.344148Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.288211Z"
    },
    "papermill": {
     "duration": 0.035212,
     "end_time": "2021-09-29T21:27:32.344742",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.309530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e79b57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.391214Z",
     "iopub.status.busy": "2021-09-29T21:27:32.390463Z",
     "iopub.status.idle": "2021-09-29T21:27:32.392768Z",
     "shell.execute_reply": "2021-09-29T21:27:32.393171Z",
     "shell.execute_reply.started": "2021-08-31T13:36:04.307378Z"
    },
    "papermill": {
     "duration": 0.029847,
     "end_time": "2021-09-29T21:27:32.393308",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.363461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model0(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n",
    "    inp = tf.keras.layers.Input(shape=(size, size, 3), name=\"inp1\")\n",
    "    label = tf.keras.layers.Input(shape=(), name=\"inp2\")\n",
    "    x = getattr(efn, f\"EfficientNetB{efficientnet_size}\")(\n",
    "        weights=weights, include_top=False, input_shape=(size, size, 3))(inp)\n",
    "    x = GeM(8)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, name=\"dense_before_arcface\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = ArcMarginProduct(\n",
    "        n_classes=N_CLASSES,\n",
    "        s=30,\n",
    "        m=0.5,\n",
    "        name=\"head/arc_margin\",\n",
    "        dtype=\"float32\"\n",
    "    )([x, label])\n",
    "    output = tf.keras.layers.Softmax(dtype=\"float32\")(x)\n",
    "    model = tf.keras.Model(inputs=[inp, label], outputs=[output])\n",
    "    opt = tf.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496f1ed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.634483Z",
     "iopub.status.busy": "2021-09-29T21:27:32.632728Z",
     "iopub.status.idle": "2021-09-29T21:27:32.635187Z",
     "shell.execute_reply": "2021-09-29T21:27:32.635643Z"
    },
    "papermill": {
     "duration": 0.223486,
     "end_time": "2021-09-29T21:27:32.635798",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.412312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_s(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n",
    "    inp = tf.keras.layers.Input(shape=(size, size, 3), name=\"inp1\")\n",
    "    label = tf.keras.layers.Input(shape=(), name=\"inp2\")\n",
    "    \n",
    "    #x = getattr(efn, f\"EfficientNetB{efficientnet_size}\")(\n",
    "    #    weights=weights, include_top=False, input_shape=(size, size, 3))(inp)\n",
    "\n",
    "    #x = keras_efficientnet_v2.EfficientNetV2S(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=\"imagenet21k-ft1k\")(inp)\n",
    "    #x = EfficientNetV2S(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=\"imagenet21k-ft1k\")(inp)\n",
    "    x = EfficientNetV2S(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=weights)(inp)\n",
    "\n",
    "    x = GeM(8)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, name=\"dense_before_arcface\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = ArcMarginProduct(\n",
    "        n_classes=N_CLASSES,\n",
    "        s=30,\n",
    "        m=0.5,\n",
    "        name=\"head/arc_margin\",\n",
    "        dtype=\"float32\"\n",
    "    )([x, label])\n",
    "    output = tf.keras.layers.Softmax(dtype=\"float32\")(x)\n",
    "    model = tf.keras.Model(inputs=[inp, label], outputs=[output])\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n",
    "    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_model_m(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n",
    "    inp = tf.keras.layers.Input(shape=(size, size, 3), name=\"inp1\")\n",
    "    label = tf.keras.layers.Input(shape=(), name=\"inp2\")\n",
    "    \n",
    "    #x = getattr(efn, f\"EfficientNetB{efficientnet_size}\")(\n",
    "    #    weights=weights, include_top=False, input_shape=(size, size, 3))(inp)\n",
    "\n",
    "    #x = keras_efficientnet_v2.EfficientNetV2S(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=\"imagenet21k-ft1k\")(inp)\n",
    "    #x = EfficientNetV2S(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=\"imagenet21k-ft1k\")(inp)\n",
    "    x = EfficientNetV2M(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=weights)(inp)\n",
    "\n",
    "    x = GeM(8)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, name=\"dense_before_arcface\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = ArcMarginProduct(\n",
    "        n_classes=N_CLASSES,\n",
    "        s=30,\n",
    "        m=0.5,\n",
    "        name=\"head/arc_margin\",\n",
    "        dtype=\"float32\"\n",
    "    )([x, label])\n",
    "    output = tf.keras.layers.Softmax(dtype=\"float32\")(x)\n",
    "    model = tf.keras.Model(inputs=[inp, label], outputs=[output])\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n",
    "    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_model_l(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n",
    "    inp = tf.keras.layers.Input(shape=(size, size, 3), name=\"inp1\")\n",
    "    label = tf.keras.layers.Input(shape=(), name=\"inp2\")\n",
    "    \n",
    "    #x = getattr(efn, f\"EfficientNetB{efficientnet_size}\")(\n",
    "    #    weights=weights, include_top=False, input_shape=(size, size, 3))(inp)\n",
    "\n",
    "    #x = keras_efficientnet_v2.EfficientNetV2S(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=\"imagenet21k-ft1k\")(inp)\n",
    "    #x = EfficientNetV2S(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=\"imagenet21k-ft1k\")(inp)\n",
    "    x = EfficientNetV2L(input_shape=(size, size, 3), drop_connect_rate=0.2, num_classes=0, pretrained=weights)(inp)\n",
    "\n",
    "    x = GeM(8)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, name=\"dense_before_arcface\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = ArcMarginProduct(\n",
    "        n_classes=N_CLASSES,\n",
    "        s=30,\n",
    "        m=0.5,\n",
    "        name=\"head/arc_margin\",\n",
    "        dtype=\"float32\"\n",
    "    )([x, label])\n",
    "    output = tf.keras.layers.Softmax(dtype=\"float32\")(x)\n",
    "    model = tf.keras.Model(inputs=[inp, label], outputs=[output])\n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(1e-3, count)\n",
    "    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "831e43e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.682018Z",
     "iopub.status.busy": "2021-09-29T21:27:32.681135Z",
     "iopub.status.idle": "2021-09-29T21:27:32.683797Z",
     "shell.execute_reply": "2021-09-29T21:27:32.683286Z",
     "shell.execute_reply.started": "2021-08-31T13:37:54.296755Z"
    },
    "papermill": {
     "duration": 0.027877,
     "end_time": "2021-09-29T21:27:32.683909",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.656032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_for_inference0(weights_path: str, efficientnet_size=5):\n",
    "    with strategy.scope():\n",
    "        base_model = build_model(\n",
    "            size=256,\n",
    "            efficientnet_size=efficientnet_size,  # 5, # 4,\n",
    "            weights=None,\n",
    "            count=0)\n",
    "        base_model.load_weights(weights_path)\n",
    "        model = tf.keras.Model(inputs=base_model.get_layer(\"inp1\").input,\n",
    "                               outputs=base_model.get_layer(\"dense_before_arcface\").output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cf5fbbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.734261Z",
     "iopub.status.busy": "2021-09-29T21:27:32.733396Z",
     "iopub.status.idle": "2021-09-29T21:27:32.735935Z",
     "shell.execute_reply": "2021-09-29T21:27:32.735494Z"
    },
    "papermill": {
     "duration": 0.032715,
     "end_time": "2021-09-29T21:27:32.736045",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.703330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_for_inference(weights_path: str, efficientnet_size=5):\n",
    "    with strategy.scope():\n",
    "        base_model = build_model(\n",
    "            size=256,\n",
    "            efficientnet_size=efficientnet_size,  # 5, # 4,\n",
    "            weights=None,\n",
    "            count=0)\n",
    "        base_model.load_weights(weights_path)\n",
    "        model = tf.keras.Model(inputs=base_model.get_layer(\"inp1\").input,\n",
    "                               outputs=base_model.get_layer(\"dense_before_arcface\").output)\n",
    "        return model\n",
    "\n",
    "    \n",
    "def create_model_s_for_inference(weights_path: str, efficientnet_size=5):\n",
    "    with strategy.scope():\n",
    "        base_model = build_model_s(\n",
    "            size=256,\n",
    "            efficientnet_size=efficientnet_size,  # 5, # 4,\n",
    "            weights=None,\n",
    "            count=0)\n",
    "        base_model.load_weights(weights_path)\n",
    "        model = tf.keras.Model(inputs=base_model.get_layer(\"inp1\").input,\n",
    "                               outputs=base_model.get_layer(\"dense_before_arcface\").output)\n",
    "        return model\n",
    "    \n",
    "def create_model_m_for_inference(weights_path: str, efficientnet_size=5):\n",
    "    with strategy.scope():\n",
    "        base_model = build_model_m(\n",
    "            size=256,\n",
    "            efficientnet_size=efficientnet_size,  # 5, # 4,\n",
    "            weights=None,\n",
    "            count=0)\n",
    "        base_model.load_weights(weights_path)\n",
    "        model = tf.keras.Model(inputs=base_model.get_layer(\"inp1\").input,\n",
    "                               outputs=base_model.get_layer(\"dense_before_arcface\").output)\n",
    "        return model\n",
    "    \n",
    "def create_model_l_for_inference(weights_path: str, im_size=256, efficientnet_size=5):\n",
    "    with strategy.scope():\n",
    "        base_model = build_model_l(\n",
    "            size=im_size, # 256,\n",
    "            efficientnet_size=efficientnet_size,  # 5, # 4,\n",
    "            weights=None,\n",
    "            count=0)\n",
    "        base_model.load_weights(weights_path)\n",
    "        model = tf.keras.Model(inputs=base_model.get_layer(\"inp1\").input,\n",
    "                               outputs=base_model.get_layer(\"dense_before_arcface\").output)\n",
    "        return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0e8ce",
   "metadata": {
    "papermill": {
     "duration": 0.018848,
     "end_time": "2021-09-29T21:27:32.774035",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.755187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90895a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.819712Z",
     "iopub.status.busy": "2021-09-29T21:27:32.818986Z",
     "iopub.status.idle": "2021-09-29T21:27:32.821724Z",
     "shell.execute_reply": "2021-09-29T21:27:32.821292Z",
     "shell.execute_reply.started": "2021-08-31T13:38:00.630554Z"
    },
    "papermill": {
     "duration": 0.028626,
     "end_time": "2021-09-29T21:27:32.821826",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.793200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_hex(image_id) -> str:\n",
    "    return '{0:0{1}x}'.format(image_id, 16)\n",
    "\n",
    "\n",
    "def get_image_path(subset, image_id):\n",
    "    name = to_hex(image_id)\n",
    "    return os.path.join(DATASET_DIR, subset, name[0], name[1], name[2], '{}.jpg'.format(name))\n",
    "\n",
    "\n",
    "def load_image_tensor(image_path, im_size=256):\n",
    "    tensor = tf.convert_to_tensor(np.array(Image.open(image_path).convert(\"RGB\")))\n",
    "    tensor = tf.image.resize(tensor, size=(im_size, im_size)) # , method='bilinear')   # , method='lanczos3')\n",
    "    tensor = tf.expand_dims(tensor, axis=0)\n",
    "    return tf.cast(tensor, tf.float32) / 255.0\n",
    "\n",
    "\n",
    "def create_batch(files, im_size=256):\n",
    "    images = []\n",
    "    for f in files:\n",
    "        images.append(load_image_tensor(f, im_size=im_size))\n",
    "    return tf.concat(images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "568c396b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.879064Z",
     "iopub.status.busy": "2021-09-29T21:27:32.873236Z",
     "iopub.status.idle": "2021-09-29T21:27:32.884444Z",
     "shell.execute_reply": "2021-09-29T21:27:32.883988Z",
     "shell.execute_reply.started": "2021-08-31T13:46:54.028912Z"
    },
    "papermill": {
     "duration": 0.043673,
     "end_time": "2021-09-29T21:27:32.884549",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.840876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_global_features(image_root_dir, n_models=4):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(image_root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                 image_paths.append(os.path.join(root, file))\n",
    "      \n",
    "    num_embeddings = len(image_paths)\n",
    "\n",
    "    ids = num_embeddings * [None]\n",
    "    ids = []\n",
    "    for path in image_paths:\n",
    "        ids.append(path.split('/')[-1][:-4])\n",
    "    \n",
    "    embeddings = np.zeros((num_embeddings, 512))\n",
    "    image_paths = np.array(image_paths)\n",
    "    chunk_size = 128 # 256 # 512\n",
    "    \n",
    "    n_chunks = len(image_paths) // chunk_size\n",
    "    if len(image_paths) % chunk_size != 0:\n",
    "        n_chunks += 1\n",
    "\n",
    "    n_models =  1 # 4  # 4  # 4 # 3\n",
    "    for n in range(n_models):\n",
    "        print(f\"Getting Embedding for fold{n} model.\")\n",
    "        #\"\"\"\n",
    "        lmodels_512 = [\n",
    "            \"../input/googlelmreffv2l512/2021_google_lm_retrieval_eff_v2l_v01_512_v02_fold3_ep57.h5\"\n",
    "        ]\n",
    "        test_im_size = 512\n",
    "        model = create_model_l_for_inference(lmodels_512[n], im_size=test_im_size)\n",
    "        \n",
    "        for i in tqdm(range(n_chunks)):\n",
    "            files = image_paths[i * chunk_size:(i + 1) * chunk_size]\n",
    "            batch = create_batch(files,im_size=test_im_size)  #  512)\n",
    "            embedding_tensor = tf.math.l2_normalize(model.predict(batch), axis = 1)\n",
    "            embeddings[i * chunk_size:(i + 1) * chunk_size] += embedding_tensor * (0.225 / n_models)\n",
    "        del model\n",
    "        gc.collect()     \n",
    "        #\"\"\"\n",
    "        \n",
    "    n_models = -4 # 8 # 4 # 3 # 4 # 3\n",
    "    for n in range(n_models):\n",
    "        print(f\"Getting Embedding for fold{n} model.\")\n",
    "        \n",
    "        lmodels_384 = [\n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold0_ep38.h5\",\n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold0_ep39.h5\", # 0.311\n",
    "            \n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold1_ep38.h5\",\n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold1_ep39.h5\", # 0.317\n",
    "            \n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold2_ep41.h5\",\n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold2_ep42.h5\", # 0.309\n",
    "            \n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold3_ep37.h5\",  #\n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold3_ep41.h5\"   # 0.322\n",
    "        ]\n",
    "        model = create_model_l_for_inference(lmodels_384[n], im_size=384)\n",
    "\n",
    "        for i in tqdm(range(n_chunks)):\n",
    "            files = image_paths[i * chunk_size:(i + 1) * chunk_size]\n",
    "            batch = create_batch(files,im_size=384)\n",
    "            embedding_tensor = tf.math.l2_normalize(model.predict(batch), axis = 1)\n",
    "            embeddings[i * chunk_size:(i + 1) * chunk_size] += embedding_tensor * (0.275 / n_models)\n",
    "        del model\n",
    "        gc.collect()     \n",
    "        \n",
    "        \n",
    "        lmodels_384 = [\n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold0_ep38.h5\",\n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold0_ep39.h5\", # 0.311\n",
    "            \n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold1_ep38.h5\",\n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold1_ep39.h5\", # 0.317\n",
    "            \n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold2_ep41.h5\",\n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold2_ep42.h5\", # 0.309\n",
    "            \n",
    "            \"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold3_ep37.h5\",  #\n",
    "            #\"../input/googlelmreffv2l384/2021_google_lm_retrieval_eff_v2l_384_v01_fold3_ep41.h5\"   # 0.322\n",
    "        ]\n",
    "        model = create_model_l_for_inference(lmodels_384[n], im_size=384)\n",
    "\n",
    "        for i in tqdm(range(n_chunks)):\n",
    "            files = image_paths[i * chunk_size:(i + 1) * chunk_size]\n",
    "            batch = create_batch(files,im_size=384)\n",
    "            embedding_tensor = tf.math.l2_normalize(model.predict(batch), axis = 1)\n",
    "            embeddings[i * chunk_size:(i + 1) * chunk_size] += embedding_tensor * (0.275 / n_models)\n",
    "        del model\n",
    "        gc.collect()     \n",
    "        \n",
    "        \n",
    "        # ../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold0.h5\n",
    "        # '../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold3.h5'\n",
    "        \n",
    "        #\"\"\"\n",
    "        #model = create_model_l_for_inference(f\"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold{n}.h5\", efficientnet_size=6)\n",
    "        #model = create_model_l_for_inference(f\"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_256_v02_fold3.h5\", efficientnet_size=6)\n",
    "        \n",
    "        lmodels = [\n",
    "            \"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold0.h5\",\n",
    "            \"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold1.h5\",\n",
    "            \"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold2.h5\",\n",
    "            \"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_256_v02_fold3.h5\",   # 0.306\n",
    "            \n",
    "            \"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold3.h5\",       # 0.303\n",
    "            \"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold3_ep25.h5\",  # 0.295\n",
    "            \"../input/googlelmreffv2l256/2021_google_lm_retrieval_eff_v2l_v02_fold3_ep30.h5\"   # 0.301\n",
    "        ]\n",
    "        model = create_model_l_for_inference(lmodels[n])\n",
    "        \n",
    "        #model = create_model_l_for_inference(lmodels[1])\n",
    "        for i in tqdm(range(n_chunks)):\n",
    "            files = image_paths[i * chunk_size:(i + 1) * chunk_size]\n",
    "            batch = create_batch(files)\n",
    "            embedding_tensor = tf.math.l2_normalize(model.predict(batch), axis = 1)\n",
    "            embeddings[i * chunk_size:(i + 1) * chunk_size] += embedding_tensor * (0.28 / n_models)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        #\"\"\"\n",
    "        \n",
    "        #model = create_model_for_inference(f\"../input/covid-tweet/fold{n}.h5\")\n",
    "        #model = create_model_for_inference(f\"../input/data-glret2021-eff3/fold{n}.h5\")\n",
    "        # ../input/google-lmr-eff5-256-v01\n",
    "        # ../input/google-lmr-eff5-256-v01/2021_google_lm_retrieval_eff5_256_v01_eff5_v01_fold0.h5\n",
    "        #model = create_model_for_inference(f\"../input/google-lmr-eff5-256-v01/2021_google_lm_retrieval_eff5_256_v01_eff5_v01_fold{n}.h5\")\n",
    "        \n",
    "        #\"\"\"\n",
    "        # ../input/google-lmr-eff6-256-v01/2021_google_lm_retrieval_eff6_v02_fold0.h5\n",
    "        \n",
    "        #model = create_model_for_inference(f\"../input/google-lmr-eff6-256-v01/2021_google_lm_retrieval_eff6_v02_fold{n}.h5\", efficientnet_size=6)   \n",
    "        #model = create_model_for_inference(f\"../input/googlelmreffv2s256/2021_google_lm_retrieval_eff_v2s_v01_fold{n}.h5\", efficientnet_size=6)\n",
    "        #model = create_model_for_inference(f\"../input/googlelmreffv2s256/2021_google_lm_retrieval_eff_v2s_v02_fold{n}.h5\", efficientnet_size=6)\n",
    "        #model = create_model_for_inference(f\"../input/googlelmreffv2s256/2021_google_lm_retrieval_eff_v2s_v03_fold{n}.h5\", efficientnet_size=6)\n",
    "        \n",
    "        # ../input/googlelmreffv2m256/2021_google_lm_retrieval_eff_v2m_v01_fold3.h5\n",
    "        #model = create_model_m_for_inference(f\"../input/../input/googlelmreffv2m256/2021_google_lm_retrieval_eff_v2m_v01_fold{n}.h5\", efficientnet_size=6)\n",
    "        model = create_model_m_for_inference(f\"../input/googlelmreffv2m256/2021_google_lm_retrieval_eff_v2m_v02_fold{n}.h5\", efficientnet_size=6)\n",
    "        #model = create_model_m_for_inference(f\"../input/../input/googlelmreffv2m256/2021_google_lm_retrieval_eff_v2m_v03_fold{n}.h5\", efficientnet_size=6)\n",
    "\n",
    "        for i in tqdm(range(n_chunks)):\n",
    "            files = image_paths[i * chunk_size:(i + 1) * chunk_size]\n",
    "            batch = create_batch(files)\n",
    "            embedding_tensor = tf.math.l2_normalize(model.predict(batch), axis = 1)\n",
    "            embeddings[i * chunk_size:(i + 1) * chunk_size] += embedding_tensor * (0.12 / n_models)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        #\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        #model = create_model_for_inference(f\"../input/google-lmr-eff5-256-v01/2021_google_lm_retrieval_eff5_256_v01_eff5_v01_fold{n}.h5\")\n",
    "        \n",
    "        model = create_model_s_for_inference(f\"../input/googlelmreffv2s256/2021_google_lm_retrieval_eff_v2s_v02_fold{n}.h5\", efficientnet_size=6)\n",
    "        #model = create_model_m_for_inference(f\"../input/../input/googlelmreffv2m256/2021_google_lm_retrieval_eff_v2m_v03_fold{n}.h5\", efficientnet_size=6)\n",
    "\n",
    "        for i in tqdm(range(n_chunks)):\n",
    "            files = image_paths[i * chunk_size:(i + 1) * chunk_size]\n",
    "            batch = create_batch(files)\n",
    "            embedding_tensor = tf.math.l2_normalize(model.predict(batch), axis = 1)\n",
    "            embeddings[i * chunk_size:(i + 1) * chunk_size] += embedding_tensor * (0.137 / n_models)\n",
    "        del model\n",
    "        gc.collect()       \n",
    "        #        #model = create_model_for_inference(f\"../input/googlelmreffv2s256/2021_google_lm_retrieval_eff_v2s_v03_fold{n}.h5\", efficientnet_size=6)\n",
    "        \"\"\"\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)    # normalize(embeddings, axis=1)\n",
    "\n",
    "    return ids, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095ef7b",
   "metadata": {
    "papermill": {
     "duration": 0.01899,
     "end_time": "2021-09-29T21:27:32.922742",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.903752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f5e564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:32.971125Z",
     "iopub.status.busy": "2021-09-29T21:27:32.970361Z",
     "iopub.status.idle": "2021-09-29T21:27:32.972648Z",
     "shell.execute_reply": "2021-09-29T21:27:32.973044Z",
     "shell.execute_reply.started": "2021-08-31T13:46:55.422524Z"
    },
    "papermill": {
     "duration": 0.031274,
     "end_time": "2021-09-29T21:27:32.973168",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.941894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions():\n",
    "    with timer(\"Getting Test Embeddings\"):\n",
    "        test_ids, test_embeddings = extract_global_features(str(TEST_IMAGE_DIR))\n",
    "\n",
    "    with timer(\"Getting Train Embeddings\"):\n",
    "        train_ids, train_embeddings = extract_global_features(str(TRAIN_IMAGE_DIR))\n",
    "\n",
    "    PredictionString_list = []\n",
    "    with timer(\"Matching...\"):\n",
    "        for test_index in range(test_embeddings.shape[0]):\n",
    "            distances = spatial.distance.cdist(test_embeddings[np.newaxis, test_index, :], train_embeddings, 'cosine')[0]\n",
    "            partition = np.argpartition(distances, TOPK)[:TOPK]\n",
    "            nearest = sorted([(train_ids[p], distances[p]) for p in partition], key=lambda x: x[1])\n",
    "            pred_str = \"\"\n",
    "            for train_id, cosine_distance in nearest:\n",
    "                pred_str += train_id\n",
    "                pred_str += \" \"\n",
    "            PredictionString_list.append(pred_str)\n",
    "\n",
    "    return test_ids, PredictionString_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_image_list = []\n",
    "    for root, dirs, files in os.walk(str(TEST_IMAGE_DIR)):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg'):\n",
    "                 test_image_list.append(os.path.join(root, file))\n",
    "    #\"\"\"                \n",
    "    if len(test_image_list)==1129:\n",
    "        sub_df = pd.read_csv('../input/landmark-retrieval-2021/sample_submission.csv')\n",
    "        sub_df.to_csv('submission.csv', index=False)\n",
    "        return\n",
    "    #\"\"\"\n",
    "    test_ids, PredictionString_list = get_predictions()\n",
    "    sub_df = pd.DataFrame(data={'id': test_ids, 'images': PredictionString_list})\n",
    "    sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f882ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-29T21:27:33.014773Z",
     "iopub.status.busy": "2021-09-29T21:27:33.014223Z",
     "iopub.status.idle": "2021-09-29T21:27:35.153935Z",
     "shell.execute_reply": "2021-09-29T21:27:35.152771Z"
    },
    "papermill": {
     "duration": 2.161808,
     "end_time": "2021-09-29T21:27:35.154080",
     "exception": false,
     "start_time": "2021-09-29T21:27:32.992272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c83c8",
   "metadata": {
    "papermill": {
     "duration": 0.018926,
     "end_time": "2021-09-29T21:27:35.193308",
     "exception": false,
     "start_time": "2021-09-29T21:27:35.174382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.644451,
   "end_time": "2021-09-29T21:27:37.279819",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-29T21:26:22.635368",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
